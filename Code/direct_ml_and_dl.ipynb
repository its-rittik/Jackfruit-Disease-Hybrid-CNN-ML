{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Direct DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T17:12:46.599813Z",
     "iopub.status.busy": "2025-10-02T17:12:46.599465Z",
     "iopub.status.idle": "2025-10-02T17:12:47.847732Z",
     "shell.execute_reply": "2025-10-02T17:12:47.846917Z",
     "shell.execute_reply.started": "2025-10-02T17:12:46.599791Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipped folder saved to: /kaggle/working/apt/ensemble_model.zip\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, Tuple, List\n",
    "from itertools import combinations\n",
    "from functools import partial\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, classification_report, confusion_matrix,\n",
    "    log_loss, roc_curve, precision_recall_curve, auc\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, label_binarize\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "HAS_XGB = False\n",
    "HAS_LGBM = False\n",
    "HAS_CAT = False\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    HAS_XGB = True\n",
    "except ImportError:\n",
    "    pass\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    HAS_LGBM = True\n",
    "except ImportError:\n",
    "    pass\n",
    "try:\n",
    "    from catboost import CatBoostClassifier\n",
    "    HAS_CAT = True\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "CSV_PATH = \"/kaggle/working/features_256d_efficientnet.csv\"\n",
    "ROOT_DIR = \"/kaggle/working/Customized CNN\"\n",
    "OUT_DIR  = \"/kaggle/working/Ensemble Model All Version/Version3\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "SAVED_MODELS = {\n",
    "    \"svm\": f\"{ROOT_DIR}/SVM/svm_final_trainval.joblib\",\n",
    "    \"xgb\": f\"{ROOT_DIR}/XGB/xgb_final_trainval.joblib\",\n",
    "    \"rf\" : f\"{ROOT_DIR}/RF/rf_final_trainval.joblib\",\n",
    "    \"knn\": f\"{ROOT_DIR}/KNN/knn_final_trainval.joblib\",\n",
    "    \"cat\": f\"{ROOT_DIR}/CAT/cat_final_trainval.cbm\",\n",
    "}\n",
    "\n",
    "rng = 42\n",
    "\n",
    "\n",
    "def setup_plot_style():\n",
    "    plt.rcParams.update({\n",
    "        \"font.family\": \"Times New Roman\", \"font.size\": 12, \"axes.labelsize\": 16,\n",
    "        \"axes.titlesize\": 18, \"font.weight\": \"bold\", \"axes.labelweight\": \"bold\"\n",
    "    })\n",
    "\n",
    "def plot_confusion_matrix(cm: np.ndarray, classes: List[str], output_path: str, title: str):\n",
    "    setup_plot_style()\n",
    "    cm_df = pd.DataFrame(cm, index=classes, columns=classes)\n",
    "    def wrap(lbl):\n",
    "        p = str(lbl).split(); return lbl if len(p) <= 1 else p[0] + \"\\n\" + \" \".join(p[1:])\n",
    "    labels = [wrap(c) for c in cm_df.columns]\n",
    "    cmap_teal = LinearSegmentedColormap.from_list(\"tealgrad\", [\"#d9f0f3\", \"#007c7c\"], N=256)\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        cm_df, annot=True, fmt=\"d\", cmap=cmap_teal, cbar=True,\n",
    "        xticklabels=labels, yticklabels=labels, linewidths=1,\n",
    "        linecolor=\"white\", annot_kws={\"fontsize\": 14, \"weight\": \"bold\"}, ax=ax\n",
    "    )\n",
    "    ax.set_title(title, weight=\"bold\")\n",
    "    ax.set_xlabel(\"Predicted\", weight=\"bold\"); ax.set_ylabel(\"Actual\", weight=\"bold\")\n",
    "    for label in ax.get_xticklabels() + ax.get_yticklabels(): label.set_fontweight(\"bold\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(output_path, dpi=600, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    print(f\"Saved confusion matrix to {output_path}\")\n",
    "\n",
    "def plot_roc_pr_curves(y_true_bin: np.ndarray, y_pred_proba: np.ndarray, classes: List[str], out_dir: str, prefix: str):\n",
    "    setup_plot_style()\n",
    "    n_classes = len(classes)\n",
    "\n",
    "    fig_roc, ax_roc = plt.subplots(figsize=(10, 8))\n",
    "    for i in range(n_classes):\n",
    "        fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_pred_proba[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        ax_roc.plot(fpr, tpr, lw=2, label=f'{classes[i]} (AUC = {roc_auc:0.2f})')\n",
    "    ax_roc.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    ax_roc.set_xlim([0.0, 1.0]); ax_roc.set_ylim([0.0, 1.05])\n",
    "    ax_roc.set_xlabel('False Positive Rate'); ax_roc.set_ylabel('True Positive Rate')\n",
    "    ax_roc.set_title(f'Receiver Operating Characteristic (ROC) - {prefix}')\n",
    "    ax_roc.legend(loc=\"lower right\", fontsize=10)\n",
    "    fig_roc.tight_layout()\n",
    "    roc_path = os.path.join(out_dir, f\"roc_curve_{prefix}.png\")\n",
    "    fig_roc.savefig(roc_path, dpi=600, bbox_inches=\"tight\")\n",
    "    plt.close(fig_roc)\n",
    "    print(f\"Saved ROC curve to {roc_path}\")\n",
    "\n",
    "    fig_pr, ax_pr = plt.subplots(figsize=(10, 8))\n",
    "    for i in range(n_classes):\n",
    "        precision, recall, _ = precision_recall_curve(y_true_bin[:, i], y_pred_proba[:, i])\n",
    "        ax_pr.plot(recall, precision, lw=2, label=f'{classes[i]}')\n",
    "    ax_pr.set_xlim([0.0, 1.0]); ax_pr.set_ylim([0.0, 1.05])\n",
    "    ax_pr.set_xlabel('Recall'); ax_pr.set_ylabel('Precision')\n",
    "    ax_pr.set_title(f'Precision-Recall Curve - {prefix}')\n",
    "    ax_pr.legend(loc=\"best\", fontsize=10)\n",
    "    fig_pr.tight_layout()\n",
    "    pr_path = os.path.join(out_dir, f\"pr_curve_{prefix}.png\")\n",
    "    fig_pr.savefig(pr_path, dpi=600, bbox_inches=\"tight\")\n",
    "    plt.close(fig_pr)\n",
    "    print(f\"Saved PR curve to {pr_path}\")\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "feat_cols = [c for c in df.columns if c.startswith(\"f\") and c[1:].isdigit()]\n",
    "if not feat_cols: raise RuntimeError(\"No feature columns (f0, f1, ...) found.\")\n",
    "X = df[feat_cols].values.astype(np.float32)\n",
    "y = df[\"class_idx\"].values.astype(int)\n",
    "classes = df.sort_values(\"class_idx\")[\"label\"].unique().tolist()\n",
    "n_classes = len(classes)\n",
    "\n",
    "X_trval, X_test, y_trval, y_test = train_test_split(X, y, test_size=0.20, stratify=y, random_state=rng)\n",
    "print(\"Split sizes:\", {\"trainval\": len(y_trval), \"test\": len(y_test)})\n",
    "\n",
    "\n",
    "def softmax(z: np.ndarray) -> np.ndarray:\n",
    "    z = z - np.max(z, axis=1, keepdims=True)\n",
    "    ez = np.exp(z)\n",
    "    return ez / np.sum(ez, axis=1, keepdims=True)\n",
    "\n",
    "def predict_logits(model, Xs) -> np.ndarray:\n",
    "    if hasattr(model, \"decision_function\"):\n",
    "        d = model.decision_function(Xs)\n",
    "        if d.ndim == 1: d = np.vstack([-d, d]).T\n",
    "        return np.asarray(d, dtype=np.float64)\n",
    "    elif hasattr(model, \"predict_proba\"):\n",
    "        return np.log(np.clip(model.predict_proba(Xs), 1e-12, 1.0))\n",
    "    else:\n",
    "        pred = model.predict(Xs)\n",
    "        L = np.full((Xs.shape[0], n_classes), -10.0, dtype=np.float64)\n",
    "        L[np.arange(Xs.shape[0]), pred] = 10.0\n",
    "        return L\n",
    "\n",
    "def predict_proba_safe(model, Xs) -> np.ndarray:\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        return np.asarray(model.predict_proba(Xs), dtype=np.float64)\n",
    "    return softmax(predict_logits(model, Xs))\n",
    "\n",
    "def entropy_margin(probs: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    p = np.clip(probs, 1e-12, 1.0)\n",
    "    ent = -(p * np.log(p)).sum(axis=1, keepdims=True)\n",
    "    top2 = np.partition(p, -2, axis=1)[:, -2:]\n",
    "    mar = (top2[:, 1] - top2[:, 0]).reshape(-1, 1)\n",
    "    return ent, mar\n",
    "\n",
    "def js_divergence(p: np.ndarray, q: np.ndarray) -> np.ndarray:\n",
    "    p = np.clip(p, 1e-12, 1.0); q = np.clip(q, 1e-12, 1.0)\n",
    "    m = 0.5 * (p + q)\n",
    "    kl_pm = (p * (np.log(p) - np.log(m))).sum(axis=1)\n",
    "    kl_qm = (q * (np.log(q) - np.log(m))).sum(axis=1)\n",
    "    return (0.5 * (kl_pm + kl_qm)).reshape(-1, 1)\n",
    "\n",
    "def disagreement_count(prob_blocks: Dict[str, np.ndarray]) -> np.ndarray:\n",
    "    preds = np.stack([np.argmax(v, axis=1) for v in prob_blocks.values()], axis=1)\n",
    "    out = [((row != np.unique(row, return_counts=True)[0][np.argmax(np.unique(row, return_counts=True)[1])]).sum(),) for row in preds]\n",
    "    return np.array(out, dtype=np.float64)\n",
    "\n",
    "def temperature_from_logits(logits: np.ndarray, y_true: np.ndarray) -> float:\n",
    "    def nll(T):\n",
    "        return log_loss(y_true, softmax(logits / float(T[0])), labels=np.arange(n_classes))\n",
    "    r = minimize(nll, x0=[1.0], bounds=[(0.2, 5.0)], method=\"L-BFGS-B\")\n",
    "    return float(np.clip(r.x[0], 0.2, 5.0))\n",
    "\n",
    "def make_base(name: str):\n",
    "    if name == \"svm\": return Pipeline([(\"scaler\", StandardScaler()), (\"clf\", SVC(C=8.0, kernel=\"rbf\", probability=True, random_state=rng))])\n",
    "    if name == \"rf\": return RandomForestClassifier(n_estimators=600, n_jobs=-1, random_state=rng)\n",
    "    if name == \"xgb\" and HAS_XGB: return XGBClassifier(objective=\"multi:softprob\", num_class=n_classes, tree_method=\"hist\", n_estimators=800, learning_rate=0.05, max_depth=6, subsample=0.9, colsample_bytree=0.9, random_state=rng, n_jobs=-1)\n",
    "    if name == \"knn\": return Pipeline([(\"scaler\", StandardScaler()), (\"clf\", KNeighborsClassifier(n_neighbors=5, weights=\"distance\"))])\n",
    "    if name == \"cat\" and HAS_CAT: return CatBoostClassifier(loss_function=\"MultiClass\", iterations=800, depth=6, learning_rate=0.06, random_seed=rng, verbose=False)\n",
    "    if name == \"lgbm\" and HAS_LGBM: return lgb.LGBMClassifier(objective=\"multiclass\", num_class=n_classes, n_estimators=900, learning_rate=0.05, subsample=0.9, colsample_bytree=0.9, random_state=rng, n_jobs=-1, verbose=-1)\n",
    "    raise ValueError(f\"Unknown or unavailable base model: {name}\")\n",
    "\n",
    "def try_load_or_build(path: str, name: str):\n",
    "    if path and os.path.exists(path):\n",
    "        try:\n",
    "            model_obj = joblib.load(path) if not path.endswith('.cbm') else make_base(name).load_model(path)\n",
    "            print(f\"Successfully loaded '{name}' from file.\")\n",
    "            return model_obj\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load '{name}' from {path} (Reason: {e}). Retraining from scratch.\")\n",
    "    return make_base(name)\n",
    "\n",
    "base_names: List[str] = [\"svm\", \"rf\", \"knn\"]\n",
    "if HAS_XGB: base_names.insert(1, \"xgb\")\n",
    "if HAS_CAT: base_names.append(\"cat\")\n",
    "if HAS_LGBM: base_names.append(\"lgbm\")\n",
    "\n",
    "bases: Dict[str, object] = {}\n",
    "for b_name in base_names:\n",
    "    model_path = SAVED_MODELS.get(b_name, \"\")\n",
    "    est = try_load_or_build(model_path, b_name)\n",
    "    bases[b_name] = est[\"model\"] if isinstance(est, dict) and \"model\" in est else est\n",
    "print(\"Using base models:\", list(bases.keys()))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=rng)\n",
    "oof_probs = {b: np.zeros((len(y_trval), n_classes)) for b in bases}\n",
    "oof_logits = {b: np.zeros((len(y_trval), n_classes)) for b in bases}\n",
    "\n",
    "print(\"Starting Out-of-Fold predictions...\")\n",
    "for k, (tr_idx, va_idx) in enumerate(skf.split(X_trval, y_trval), 1):\n",
    "    Xtr, Xva = X_trval[tr_idx], X_trval[va_idx]\n",
    "    ytr = y_trval[tr_idx]\n",
    "    for b_name, est_template in bases.items():\n",
    "        est = make_base(b_name) \n",
    "        est.fit(Xtr, ytr)\n",
    "        oof_probs[b_name][va_idx] = predict_proba_safe(est, Xva)\n",
    "        oof_logits[b_name][va_idx] = predict_logits(est, Xva)\n",
    "    print(f\"Fold {k} complete.\")\n",
    "\n",
    "CALIBRATE = {\"svm\", \"knn\"}\n",
    "temperatures = {b: temperature_from_logits(oof_logits[b], y_trval) if b in CALIBRATE else 1.0 for b in bases}\n",
    "print(\"Optimal Temperatures:\", temperatures)\n",
    "oof_probs_cal = {b: softmax(oof_logits[b] / T) for b, T in temperatures.items()}\n",
    "\n",
    "def make_meta_features(prob_blocks: Dict[str, np.ndarray]) -> np.ndarray:\n",
    "    names = sorted(prob_blocks.keys())\n",
    "    features = [prob_blocks[n] for n in names]\n",
    "    for n in names:\n",
    "        ent, mar = entropy_margin(prob_blocks[n])\n",
    "        features.extend([ent, mar])\n",
    "    features.append(disagreement_count(prob_blocks))\n",
    "    for n1, n2 in combinations(names, 2):\n",
    "        features.append(js_divergence(prob_blocks[n1], prob_blocks[n2]))\n",
    "    return np.hstack(features)\n",
    "\n",
    "X_meta_oof = make_meta_features(oof_probs_cal)\n",
    "print(f\"OOF Meta-feature shape: {X_meta_oof.shape}\")\n",
    "\n",
    "meta_learners = {\n",
    "    \"logreg\": LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", C=1.0, random_state=rng, n_jobs=-1, max_iter=1000),\n",
    "    \"mlp\": MLPClassifier(hidden_layer_sizes=(128, 64), activation=\"relu\", solver=\"adam\", random_state=rng, max_iter=500)\n",
    "}\n",
    "if HAS_XGB: meta_learners[\"xgb\"] = XGBClassifier(objective=\"multi:softprob\", num_class=n_classes, n_estimators=500, learning_rate=0.05, max_depth=4, random_state=rng, n_jobs=-1)\n",
    "\n",
    "oof_meta_acc = {}\n",
    "for name, model in meta_learners.items():\n",
    "    model.fit(X_meta_oof, y_trval)\n",
    "    oof_preds = model.predict(X_meta_oof)\n",
    "    acc = accuracy_score(y_trval, oof_preds)\n",
    "    oof_meta_acc[name] = acc\n",
    "    print(f\"OOF Meta-learner '{name}' accuracy: {acc:.4f}\")\n",
    "best_meta_name = max(oof_meta_acc, key=oof_meta_acc.get)\n",
    "best_meta_model = meta_learners[best_meta_name]\n",
    "print(f\"Best OOF meta-learner: '{best_meta_name}'\")\n",
    "\n",
    "def nll_blend_loss(weights, prob_blocks, y_true):\n",
    "    probs = sum(w * prob_blocks[n] for w, n in zip(weights, sorted(prob_blocks.keys())))\n",
    "    return log_loss(y_true, probs, labels=np.arange(n_classes))\n",
    "\n",
    "initial_weights = np.ones(len(bases)) / len(bases)\n",
    "bounds = [(0, 1)] * len(bases)\n",
    "constraints = ({'type': 'eq', 'fun': lambda w: np.sum(w) - 1})\n",
    "res = minimize(nll_blend_loss, initial_weights, args=(oof_probs_cal, y_trval), method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "blend_weights = res.x\n",
    "blend_names = sorted(bases.keys())\n",
    "print(\"Optimized Blend Weights:\", {n: f\"{w:.4f}\" for n, w in zip(blend_names, blend_weights)})\n",
    "\n",
    "\n",
    "\n",
    "print(\"Refitting base models on full trainval set for test prediction...\")\n",
    "fitted_bases = {}\n",
    "for b_name in bases.keys(): \n",
    "    print(f\"  -> Fitting final '{b_name}' model...\")\n",
    "\n",
    "    final_model = make_base(b_name)\n",
    "    final_model.fit(X_trval, y_trval)\n",
    "    fitted_bases[b_name] = final_model\n",
    "\n",
    "test_probs = {b: predict_proba_safe(fitted_bases[b], X_test) for b in bases}\n",
    "test_logits = {b: predict_logits(fitted_bases[b], X_test) for b in bases}\n",
    "test_probs_cal = {b: softmax(test_logits[b] / temperatures[b]) for b in bases}\n",
    "\n",
    "X_meta_test = make_meta_features(test_probs_cal)\n",
    "\n",
    "ptest_meta = best_meta_model.predict_proba(X_meta_test)\n",
    "pred_meta = ptest_meta.argmax(1)\n",
    "acc_meta = accuracy_score(y_test, pred_meta)\n",
    "f1_meta = f1_score(y_test, pred_meta, average=\"macro\")\n",
    "\n",
    "ptest_blend = sum(w * test_probs_cal[n] for w, n in zip(blend_weights, blend_names))\n",
    "pred_blend = ptest_blend.argmax(1)\n",
    "acc_blend = accuracy_score(y_test, pred_blend)\n",
    "f1_blend = f1_score(y_test, pred_blend, average=\"macro\")\n",
    "\n",
    "lams, best_lam, best_mix_acc = np.linspace(0, 1, 21), 0.0, -1.0\n",
    "p_meta_oof_preds = best_meta_model.predict_proba(X_meta_oof)\n",
    "p_blend_oof = sum(w * oof_probs_cal[n] for w, n in zip(blend_weights, blend_names))\n",
    "for lam in lams:\n",
    "    acc = accuracy_score(y_trval, (lam * p_meta_oof_preds + (1 - lam) * p_blend_oof).argmax(1))\n",
    "    if acc > best_mix_acc: best_mix_acc, best_lam = acc, lam\n",
    "ptest_mix = best_lam * ptest_meta + (1 - best_lam) * ptest_blend\n",
    "pred_mix = ptest_mix.argmax(1)\n",
    "acc_mix = accuracy_score(y_test, pred_mix)\n",
    "f1_mix = f1_score(y_test, pred_mix, average=\"macro\")\n",
    "\n",
    "candidates = {\n",
    "    best_meta_name: (acc_meta, f1_meta, pred_meta, ptest_meta),\n",
    "    \"blend\": (acc_blend, f1_blend, pred_blend, ptest_blend),\n",
    "    f\"mix_{best_meta_name}_blend\": (acc_mix, f1_mix, pred_mix, ptest_mix)\n",
    "}\n",
    "winner_name = max(candidates.items(), key=lambda kv: kv[1][0])[0]\n",
    "w_acc, w_f1, w_pred, w_proba = candidates[winner_name]\n",
    "\n",
    "print(f\"\\n--- TEST RESULTS ---\")\n",
    "print(f\"{best_meta_name}: acc={acc_meta:.4f}, f1_macro={f1_meta:.4f}\")\n",
    "print(f\"Blend: acc={acc_blend:.4f}, f1_macro={f1_blend:.4f}\")\n",
    "print(f\"Mix: acc={acc_mix:.4f}, f1_macro={f1_mix:.4f} (λ={best_lam:.2f})\")\n",
    "print(f\"\\nWINNER: {winner_name} with Accuracy = {w_acc:.4f}, F1-Macro = {w_f1:.4f}\")\n",
    "\n",
    "rep = classification_report(y_test, w_pred, target_names=classes, output_dict=True)\n",
    "pd.DataFrame(rep).transpose().to_csv(os.path.join(OUT_DIR, f\"report_{winner_name}.csv\"))\n",
    "\n",
    "cm = confusion_matrix(y_test, w_pred)\n",
    "plot_confusion_matrix(cm, classes, os.path.join(OUT_DIR, f\"cm_{winner_name}.png\"), f\"Confusion Matrix - {winner_name}\")\n",
    "\n",
    "y_test_bin = label_binarize(y_test, classes=np.arange(n_classes))\n",
    "plot_roc_pr_curves(y_test_bin, w_proba, classes, OUT_DIR, winner_name)\n",
    "\n",
    "\n",
    "summary = {\n",
    "    \"ensemble_version\": \"V3\", \"winner\": winner_name,\n",
    "    \"test_metrics\": {k: {\"acc\": float(v[0]), \"f1_macro\": float(v[1])} for k, v in candidates.items()},\n",
    "    \"temperatures\": temperatures,\n",
    "    \"blend_weights\": {n: w for n, w in zip(blend_names, blend_weights)},\n",
    "    \"mix_lambda\": float(best_lam)\n",
    "}\n",
    "with open(os.path.join(OUT_DIR, \"summary_v3.json\"), \"w\") as f: json.dump(summary, f, indent=4)\n",
    "joblib.dump(best_meta_model, os.path.join(OUT_DIR, f\"meta_model_{best_meta_name}_v3.joblib\"))\n",
    "print(f\"\\nSaved all V3 artifacts to: {OUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Direct ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T19:07:38.907909Z",
     "iopub.status.busy": "2025-09-05T19:07:38.907595Z",
     "iopub.status.idle": "2025-09-05T19:10:38.178985Z",
     "shell.execute_reply": "2025-09-05T19:10:38.172946Z",
     "shell.execute_reply.started": "2025-09-05T19:07:38.907881Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 9000 images; raw dim=12288; classes=6\n",
      "{'train': 6300, 'val': 900, 'test': 1800}\n",
      "Transform: StandardScaler + PCA(256) | dim: 256\n",
      "\n",
      "=== LR_multinomial ===\n",
      "LR_multinomial -> VAL acc 0.7800 | TEST acc 0.7833\n",
      "\n",
      "=== SVM_RBF ===\n",
      "SVM_RBF -> VAL acc 0.9478 | TEST acc 0.9422\n",
      "\n",
      "=== RF ===\n",
      "RF -> VAL acc 0.9056 | TEST acc 0.9039\n",
      "\n",
      "=== XGB ===\n",
      "XGB -> VAL acc 0.9111 | TEST acc 0.9156\n",
      "\n",
      "=== KNN ===\n",
      "KNN -> VAL acc 0.8956 | TEST acc 0.8800\n",
      "\n",
      "=== CatBoost ===\n",
      "CatBoost -> VAL acc 0.9100 | TEST acc 0.9044\n",
      "\n",
      "Done. Models & reports saved in: /kaggle/working/ML_Direct\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, json, glob, joblib, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    HAS_XGB = True\n",
    "except Exception:\n",
    "    HAS_XGB = False\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostClassifier\n",
    "    HAS_CAT = True\n",
    "except Exception:\n",
    "    HAS_CAT = False\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "RNG = 42\n",
    "\n",
    "DATA_ROOT = \"/kaggle/input/jackfruit/AugmentedJackfruit\"   \n",
    "OUT_DIR   = \"/kaggle/working/ML_Direct\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "IMG_SIZE = (64, 64)    \n",
    "PCA_N    = 256         \n",
    "\n",
    "def load_images_from_root(root, size=(64,64)):\n",
    "    cls_dirs = sorted([d for d in glob.glob(os.path.join(root, \"*\")) if os.path.isdir(d)])\n",
    "    if not cls_dirs:\n",
    "        raise RuntimeError(f\"No class folders under: {root}\")\n",
    "    X_list, y_list, classes = [], [], [os.path.basename(d) for d in cls_dirs]\n",
    "    for ci, d in enumerate(cls_dirs):\n",
    "        paths = []\n",
    "        for ext in (\"*.jpg\",\"*.jpeg\",\"*.png\",\"*.bmp\",\"*.tif\",\"*.tiff\",\"*.webp\"):\n",
    "            paths += glob.glob(os.path.join(d, ext))\n",
    "        for p in paths:\n",
    "            im = Image.open(p).convert(\"RGB\").resize(size, Image.BILINEAR)\n",
    "            X_list.append(np.asarray(im, dtype=np.uint8).reshape(-1))\n",
    "            y_list.append(ci)\n",
    "    X = np.stack(X_list).astype(np.float32) / 255.0\n",
    "    y = np.array(y_list, dtype=np.int64)\n",
    "    return X, y, classes\n",
    "\n",
    "X_all, y_all, class_names = load_images_from_root(DATA_ROOT, IMG_SIZE)\n",
    "n_classes = len(np.unique(y_all))\n",
    "print(f\"Loaded {X_all.shape[0]} images; raw dim={X_all.shape[1]}; classes={n_classes}\")\n",
    "\n",
    "\n",
    "X_trv, X_test, y_trv, y_test = train_test_split(X_all, y_all, test_size=0.20, stratify=y_all, random_state=RNG)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trv, y_trv, test_size=0.125, stratify=y_trv, random_state=RNG)\n",
    "print({\"train\": len(y_train), \"val\": len(y_val), \"test\": len(y_test)})\n",
    "\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_val_s   = scaler.transform(X_val)\n",
    "X_test_s  = scaler.transform(X_test)\n",
    "\n",
    "if PCA_N is not None:\n",
    "    pca = PCA(n_components=PCA_N, random_state=RNG)\n",
    "    X_train_t = pca.fit_transform(X_train_s)\n",
    "    X_val_t   = pca.transform(X_val_s)\n",
    "    X_test_t  = pca.transform(X_test_s)\n",
    "    trans_desc = f\"StandardScaler + PCA({PCA_N})\"\n",
    "else:\n",
    "    pca = None\n",
    "    X_train_t, X_val_t, X_test_t = X_train_s, X_val_s, X_test_s\n",
    "    trans_desc = \"StandardScaler only\"\n",
    "\n",
    "print(\"Transform:\", trans_desc, \"| dim:\", X_train_t.shape[1])\n",
    "\n",
    "def eval_and_save(name, model, Xt, yt, split, out_dir):\n",
    "    y_pred = model.predict(Xt)\n",
    "    acc = accuracy_score(yt, y_pred)\n",
    "    f1m = f1_score(yt, y_pred, average=\"macro\")\n",
    "    rep = classification_report(yt, y_pred, target_names=class_names, output_dict=True)\n",
    "    cm  = confusion_matrix(yt, y_pred)\n",
    "    pd.DataFrame(rep).transpose().to_csv(os.path.join(out_dir, f\"{name}_{split}_report.csv\"))\n",
    "    pd.DataFrame(cm, index=class_names, columns=class_names).to_csv(os.path.join(out_dir, f\"{name}_{split}_cm.csv\"))\n",
    "\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.imshow(cm, interpolation='nearest'); plt.title(f\"{name} — {split} CM\"); plt.colorbar()\n",
    "    ticks = np.arange(len(class_names))\n",
    "    plt.xticks(ticks, class_names, rotation=45, ha=\"right\"); plt.yticks(ticks, class_names)\n",
    "    th = cm.max()/2\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, str(cm[i,j]), ha=\"center\", va=\"center\",\n",
    "                     color=\"white\" if cm[i,j] > th else \"black\")\n",
    "    plt.tight_layout(); plt.ylabel(\"True\"); plt.xlabel(\"Pred\")\n",
    "    plt.savefig(os.path.join(out_dir, f\"{name}_{split}_cm.png\"), dpi=160, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    return {\"accuracy\": float(acc), \"f1_macro\": float(f1m)}\n",
    "\n",
    "def save_bundle(name, model, out_dir, scaler, pca):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    if name.lower().startswith(\"catboost\"):\n",
    "        model.save_model(os.path.join(out_dir, f\"{name}.cbm\"))\n",
    "        joblib.dump({\"scaler\": scaler, \"pca\": pca, \"classes\": class_names, \"img_size\": IMG_SIZE},\n",
    "                    os.path.join(out_dir, f\"{name}_preproc.joblib\"))\n",
    "    else:\n",
    "        joblib.dump({\"model\": model, \"scaler\": scaler, \"pca\": pca, \"classes\": class_names, \"img_size\": IMG_SIZE},\n",
    "                    os.path.join(out_dir, f\"{name}.joblib\"))\n",
    "\n",
    "def train_and_report(name, model, Xt, yt, Xv, yv, Xte, yte):\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    d = os.path.join(OUT_DIR, name.replace(\" \", \"_\")); os.makedirs(d, exist_ok=True)\n",
    "    model.fit(Xt, yt)\n",
    "    val_sum  = eval_and_save(name, model, Xv,  yv,  \"val\",  d)\n",
    "    test_sum = eval_and_save(name, model, Xte, yte, \"test\", d)\n",
    "    save_bundle(name, model, d, scaler, pca)\n",
    "    with open(os.path.join(d, \"summary.json\"), \"w\") as f:\n",
    "        json.dump({\"transform\": trans_desc, \"val\": val_sum, \"test\": test_sum, \"classes\": class_names}, f, indent=2)\n",
    "    print(f\"{name} -> VAL acc {val_sum['accuracy']:.4f} | TEST acc {test_sum['accuracy']:.4f}\")\n",
    "\n",
    "lr  = LogisticRegression(C=1.0, penalty=\"l2\", solver=\"lbfgs\",\n",
    "                         multi_class=\"multinomial\", max_iter=1000, n_jobs=-1, random_state=RNG)\n",
    "train_and_report(\"LR_multinomial\", lr, X_train_t, y_train, X_val_t, y_val, X_test_t, y_test)\n",
    "\n",
    "svm = SVC(kernel=\"rbf\", C=10.0, gamma=\"scale\", probability=True, random_state=RNG)\n",
    "train_and_report(\"SVM_RBF\", svm, X_train_t, y_train, X_val_t, y_val, X_test_t, y_test)\n",
    "\n",
    "rf  = RandomForestClassifier(n_estimators=600, max_depth=None, n_jobs=-1, random_state=RNG)\n",
    "train_and_report(\"RF\", rf, X_train_t, y_train, X_val_t, y_val, X_test_t, y_test)\n",
    "\n",
    "if HAS_XGB:\n",
    "    xgb = XGBClassifier(\n",
    "        objective=\"multi:softprob\", num_class=n_classes, eval_metric=\"mlogloss\",\n",
    "        tree_method=\"hist\", n_estimators=800, learning_rate=0.05, max_depth=6,\n",
    "        subsample=0.9, colsample_bytree=0.9, reg_lambda=1.0, reg_alpha=0.0,\n",
    "        random_state=RNG\n",
    "    )\n",
    "    train_and_report(\"XGB\", xgb, X_train_t, y_train, X_val_t, y_val, X_test_t, y_test)\n",
    "else:\n",
    "    print(\"XGBoost not available — skipping.\")\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5, weights=\"distance\", n_jobs=-1)\n",
    "train_and_report(\"KNN\", knn, X_train_t, y_train, X_val_t, y_val, X_test_t, y_test)\n",
    "\n",
    "if HAS_CAT:\n",
    "    cat = CatBoostClassifier(loss_function=\"MultiClass\" if n_classes>2 else \"Logloss\",\n",
    "                             eval_metric=\"MultiClass\", iterations=600, depth=6, learning_rate=0.06,\n",
    "                             random_seed=RNG, verbose=False)\n",
    "    train_and_report(\"CatBoost\", cat, X_train_t, y_train, X_val_t, y_val, X_test_t, y_test)\n",
    "else:\n",
    "    print(\"CatBoost not available — skipping.\")\n",
    "\n",
    "print(\"\\nDone. Models & reports saved in:\", OUT_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8106950,
     "sourceId": 12820038,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
