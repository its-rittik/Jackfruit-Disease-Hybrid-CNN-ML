{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ML on Augmented DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os, json, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, label_binarize\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    classification_report, confusion_matrix, roc_curve, auc,\n",
    "    precision_recall_curve, average_precision_score\n",
    ")\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "\n",
    "\n",
    "CSV_PATH   = \"/kaggle/working/features_256d_efficientnet.csv\"\n",
    "OUT_DIR    = \"/kaggle/working/Customized CNN/XGB\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "REPORT_OUT = os.path.join(OUT_DIR, \"xgb_report.json\")\n",
    "MODEL_VAL_BEST = os.path.join(OUT_DIR, \"xgb_valbest.joblib\")\n",
    "MODEL_FINAL    = os.path.join(OUT_DIR, \"xgb_final_trainval.joblib\")\n",
    "SEARCH_CSV     = os.path.join(OUT_DIR, \"xgb_val_search_results.csv\")\n",
    "\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "feat_cols = [c for c in df.columns if c.startswith(\"f\") and c[1:].isdigit()]\n",
    "X = df[feat_cols].values.astype(np.float32)\n",
    "y = df[\"class_idx\"].values.astype(np.int64)\n",
    "\n",
    "class_map = df.sort_values(\"class_idx\")[[\"class_idx\",\"label\"]].drop_duplicates()\n",
    "class_names = class_map.set_index(\"class_idx\")[\"label\"].reindex(sorted(class_map[\"class_idx\"])).tolist()\n",
    "n_classes = len(np.unique(y))\n",
    "rng = 42\n",
    "\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, stratify=y, random_state=rng\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_trainval, y_trainval, test_size=0.125, stratify=y_trainval, random_state=rng\n",
    ")\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_s = scaler.transform(X_train)\n",
    "X_val_s   = scaler.transform(X_val)\n",
    "X_test_s  = scaler.transform(X_test)\n",
    "\n",
    "def make_xgb(params):\n",
    "    return XGBClassifier(\n",
    "        objective=\"multi:softprob\",\n",
    "        num_class=n_classes,\n",
    "        eval_metric=\"mlogloss\",\n",
    "        tree_method=\"hist\",\n",
    "        random_state=rng,\n",
    "        n_jobs=-1,\n",
    "        **params\n",
    "    )\n",
    "\n",
    "PARAMS = {\n",
    "    \"learning_rate\":     [0.03, 0.06],     \n",
    "    \"max_depth\":         [3, 4, 5],        \n",
    "    \"min_child_weight\":  [1, 3],           \n",
    "    \"subsample\":         [0.8, 1.0],\n",
    "    \"colsample_bytree\":  [0.8, 1.0],\n",
    "    \"reg_alpha\":         [0.0, 1e-3],     \n",
    "    \"reg_lambda\":        [1.0],           \n",
    "}\n",
    "\n",
    "search_rows, best = [], {\"acc\": -1, \"params\": None, \"model\": None}\n",
    "\n",
    "BASE_N_ESTIMATORS = 2000\n",
    "EARLY_STOP_ROUNDS = 50\n",
    "\n",
    "keys = list(PARAMS.keys())\n",
    "grids = [dict(zip(keys, values)) for values in product(*[PARAMS[k] for k in keys])]\n",
    "\n",
    "for p in grids:\n",
    "    params = dict(\n",
    "        n_estimators=BASE_N_ESTIMATORS,\n",
    "        **p\n",
    "    )\n",
    "    model = make_xgb(params)\n",
    "    model.fit(\n",
    "        X_train_s, y_train,\n",
    "        eval_set=[(X_val_s, y_val)],\n",
    "        verbose=False,\n",
    "        early_stopping_rounds=EARLY_STOP_ROUNDS\n",
    "    )\n",
    "    yv_pred = model.predict(X_val_s)\n",
    "    acc = accuracy_score(y_val, yv_pred)\n",
    "    f1m = f1_score(y_val, yv_pred, average=\"macro\")\n",
    "\n",
    "    search_rows.append({**p, \"best_iteration\": int(getattr(model, \"best_iteration\", model.n_estimators-1)),\n",
    "                        \"val_accuracy\": acc, \"val_f1_macro\": f1m})\n",
    "\n",
    "    if acc > best[\"acc\"]:\n",
    "        best.update({\"acc\": acc, \"params\": params, \"model\": model})\n",
    "\n",
    "\n",
    "pd.DataFrame(search_rows).sort_values([\"val_accuracy\",\"val_f1_macro\"], ascending=False).to_csv(SEARCH_CSV, index=False)\n",
    "joblib.dump({\"scaler\": scaler, \"model\": best[\"model\"]}, MODEL_VAL_BEST)\n",
    "\n",
    "\n",
    "def save_confmat_and_reports(Xs, y_true, model, prefix):\n",
    "    y_proba = model.predict_proba(Xs)\n",
    "    y_pred  = y_proba.argmax(1)\n",
    "\n",
    "    acc   = accuracy_score(y_true, y_pred)\n",
    "    f1m   = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    f1w   = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "    precm = precision_score(y_true, y_pred, average=\"macro\")\n",
    "    precw = precision_score(y_true, y_pred, average=\"weighted\")\n",
    "    recm  = recall_score(y_true, y_pred, average=\"macro\")\n",
    "    recw  = recall_score(y_true, y_pred, average=\"weighted\")\n",
    "\n",
    "    pd.DataFrame(classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
    "                 ).transpose().to_csv(os.path.join(OUT_DIR, f\"{prefix}_classification_report.csv\"))\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    pd.DataFrame(cm, index=class_names, columns=class_names).to_csv(os.path.join(OUT_DIR, f\"{prefix}_confusion_matrix.csv\"))\n",
    "    plt.figure(figsize=(6,5)); plt.imshow(cm, interpolation='nearest'); plt.title(f\"Confusion Matrix — {prefix}\")\n",
    "    plt.colorbar(); ticks=np.arange(len(class_names))\n",
    "    plt.xticks(ticks, class_names, rotation=45, ha=\"right\"); plt.yticks(ticks, class_names)\n",
    "    th=cm.max()/2\n",
    "    for i in range(cm.shape[0]):\n",
    "      for j in range(cm.shape[1]):\n",
    "        plt.text(j,i,str(cm[i,j]),ha=\"center\",va=\"center\",color=\"white\" if cm[i,j]>th else \"black\")\n",
    "    plt.tight_layout(); plt.ylabel(\"True\"); plt.xlabel(\"Pred\")\n",
    "    plt.savefig(os.path.join(OUT_DIR, f\"{prefix}_confusion_matrix.png\"), dpi=160, bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "    metrics_extra = {}\n",
    "    if n_classes > 1:\n",
    "        y_bin = label_binarize(y_true, classes=np.arange(n_classes))\n",
    "        fpr, tpr, roc_auc, prec, rec, ap = {}, {}, {}, {}, {}, {}\n",
    "        for c in range(n_classes):\n",
    "            fpr[c], tpr[c], _ = roc_curve(y_bin[:, c], y_proba[:, c]); roc_auc[c] = auc(fpr[c], tpr[c])\n",
    "            prec[c], rec[c], _ = precision_recall_curve(y_bin[:, c], y_proba[:, c]); ap[c] = average_precision_score(y_bin[:, c], y_proba[:, c])\n",
    "        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_bin.ravel(), y_proba.ravel()); roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "        prec[\"micro\"], rec[\"micro\"], _ = precision_recall_curve(y_bin.ravel(), y_proba.ravel()); ap[\"micro\"] = average_precision_score(y_bin, y_proba, average=\"micro\")\n",
    "        all_fpr = np.unique(np.concatenate([fpr[c] for c in range(n_classes)])); mean_tpr = np.zeros_like(all_fpr)\n",
    "        for c in range(n_classes): mean_tpr += np.interp(all_fpr, fpr[c], tpr[c])\n",
    "        mean_tpr /= n_classes; roc_auc[\"macro\"] = auc(all_fpr, mean_tpr); ap[\"macro\"] = np.mean([ap[c] for c in range(n_classes)])\n",
    "        rows = []; \n",
    "        for key in list(range(n_classes)) + [\"micro\"]:\n",
    "            for xi, yi in zip(fpr[key], tpr[key]): rows.append({\"curve\": f\"ROC_{key}\", \"fpr\": float(xi), \"tpr\": float(yi)})\n",
    "        for xi, yi in zip(all_fpr, mean_tpr): rows.append({\"curve\": \"ROC_macro\", \"fpr\": float(xi), \"tpr\": float(yi)})\n",
    "        pd.DataFrame(rows).to_csv(os.path.join(OUT_DIR, f\"{prefix}_roc_points.csv\"), index=False)\n",
    "        rows = []\n",
    "        for key in list(range(n_classes)) + [\"micro\"]:\n",
    "            for pi, ri in zip(prec[key], rec[key]): rows.append({\"curve\": f\"PR_{key}\", \"precision\": float(pi), \"recall\": float(ri)})\n",
    "        pd.DataFrame(rows).to_csv(os.path.join(OUT_DIR, f\"{prefix}_pr_points.csv\"), index=False)\n",
    "\n",
    "        plt.figure(figsize=(7,6))\n",
    "        for c in range(n_classes): plt.plot(fpr[c], tpr[c], lw=1.2, label=f\"{class_names[c]} (AUC={roc_auc[c]:.3f})\")\n",
    "        plt.plot(fpr[\"micro\"], tpr[\"micro\"], lw=2, linestyle=\"--\", label=f\"micro (AUC={roc_auc['micro']:.3f})\")\n",
    "        plt.plot([0,1],[0,1],\"k--\", lw=1); plt.xlim([0,1]); plt.ylim([0,1.05])\n",
    "        plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(f\"ROC — {prefix}\")\n",
    "        plt.legend(loc=\"lower right\", fontsize=8); plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUT_DIR, f\"{prefix}_roc_curves.png\"), dpi=160); plt.close()\n",
    "\n",
    "        plt.figure(figsize=(7,6))\n",
    "        for c in range(n_classes): plt.plot(rec[c], prec[c], lw=1.2, label=f\"{class_names[c]} (AP={ap[c]:.3f})\")\n",
    "        plt.plot(rec[\"micro\"], prec[\"micro\"], lw=2, linestyle=\"--\", label=f\"micro (AP={ap['micro']:.3f})\")\n",
    "        plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(f\"PR — {prefix}\")\n",
    "        plt.legend(loc=\"lower left\", fontsize=8); plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUT_DIR, f\"{prefix}_pr_curves.png\"), dpi=160); plt.close()\n",
    "\n",
    "        metrics_extra = {\n",
    "            \"roc_auc_per_class\": {class_names[c]: float(roc_auc[c]) for c in range(n_classes)},\n",
    "            \"roc_auc_micro\": float(roc_auc[\"micro\"]),\n",
    "            \"roc_auc_macro\": float(roc_auc[\"macro\"]),\n",
    "            \"ap_per_class\": {class_names[c]: float(ap[c]) for c in range(n_classes)},\n",
    "            \"ap_micro\": float(ap[\"micro\"]),\n",
    "            \"ap_macro\": float(ap[\"macro\"]),\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": float(acc),\n",
    "        \"f1_macro\": float(f1m),\n",
    "        \"f1_weighted\": float(f1w),\n",
    "        \"precision_macro\": float(precm),\n",
    "        \"precision_weighted\": float(precw),\n",
    "        \"recall_macro\": float(recm),\n",
    "        \"recall_weighted\": float(recw),\n",
    "        **metrics_extra\n",
    "    }\n",
    "\n",
    "val_summary = save_confmat_and_reports(X_val_s, y_val, best[\"model\"], prefix=\"val_xgb\")\n",
    "joblib.dump({\"scaler\": scaler, \"model\": best[\"model\"]}, MODEL_VAL_BEST)\n",
    "\n",
    "p = best[\"params\"]\n",
    "final_model = make_xgb(p)\n",
    "final_model.fit(np.vstack([X_train_s, X_val_s]), np.concatenate([y_train, y_val]), verbose=False)\n",
    "joblib.dump({\"scaler\": scaler, \"model\": final_model}, MODEL_FINAL)\n",
    "\n",
    "test_summary = save_confmat_and_reports(X_test_s, y_test, final_model, prefix=\"test_xgb\")\n",
    "\n",
    "report = {\n",
    "    \"split_sizes\": {\"train\": int(X_train.shape[0]), \"val\": int(X_val.shape[0]), \"test\": int(X_test.shape[0])},\n",
    "    \"val_search_best_params\": p,\n",
    "    \"val_metrics\": val_summary,\n",
    "    \"test_metrics\": test_summary,\n",
    "    \"classes\": class_names\n",
    "}\n",
    "with open(REPORT_OUT, \"w\") as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "print(\"\\nSaved artifacts to:\", OUT_DIR)\n",
    "print(\"Best VAL acc:\", f\"{val_summary['accuracy']:.4f}\")\n",
    "print(\"Final TEST acc:\", f\"{test_summary['accuracy']:.4f}\")\n",
    "print(\"Report:\", REPORT_OUT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import os, json, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, label_binarize\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    classification_report, confusion_matrix, roc_curve, auc,\n",
    "    precision_recall_curve, average_precision_score\n",
    ")\n",
    "import joblib\n",
    "\n",
    "CSV_PATH   = \"/kaggle/working/features_256d_efficientnet.csv\"\n",
    "OUT_DIR    = \"/kaggle/working/Customized CNN/SVM\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "REPORT_OUT = os.path.join(OUT_DIR, \"svm_report.json\")\n",
    "MODEL_VAL_BEST = os.path.join(OUT_DIR, \"svm_valbest.joblib\")\n",
    "MODEL_FINAL    = os.path.join(OUT_DIR, \"svm_final_trainval.joblib\")\n",
    "SEARCH_CSV     = os.path.join(OUT_DIR, \"svm_val_search_results.csv\")\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "feat_cols = [c for c in df.columns if c.startswith(\"f\") and c[1:].isdigit()]\n",
    "X = df[feat_cols].values.astype(np.float32)\n",
    "y = df[\"class_idx\"].values.astype(np.int64)\n",
    "\n",
    "class_map = df.sort_values(\"class_idx\")[[\"class_idx\",\"label\"]].drop_duplicates()\n",
    "class_names = class_map.set_index(\"class_idx\")[\"label\"].reindex(sorted(class_map[\"class_idx\"])).tolist()\n",
    "n_classes = len(np.unique(y))\n",
    "rng = 42\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, stratify=y, random_state=rng\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_trainval, y_trainval, test_size=0.125, stratify=y_trainval, random_state=rng\n",
    ")\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_s = scaler.transform(X_train)\n",
    "X_val_s   = scaler.transform(X_val)\n",
    "X_test_s  = scaler.transform(X_test)\n",
    "\n",
    "def make_svm(C, gamma):\n",
    "    return SVC(kernel=\"rbf\", C=C, gamma=gamma, probability=True, random_state=rng)\n",
    "\n",
    "C_LIST     = np.logspace(-2, 3, 12)    \n",
    "GAMMA_LIST = np.logspace(-5, 1, 13)  \n",
    "\n",
    "rows, best = [], {\"acc\": -1, \"params\": None, \"model\": None}\n",
    "for C in C_LIST:\n",
    "    for g in GAMMA_LIST:\n",
    "        model = make_svm(C, g)\n",
    "        model.fit(X_train_s, y_train)\n",
    "        yv_pred = model.predict(X_val_s)\n",
    "        acc = accuracy_score(y_val, yv_pred)\n",
    "        f1m = f1_score(y_val, yv_pred, average=\"macro\")\n",
    "        rows.append({\"C\": C, \"gamma\": g, \"val_accuracy\": acc, \"val_f1_macro\": f1m})\n",
    "        if acc > best[\"acc\"]:\n",
    "            best.update({\"acc\": acc, \"params\": {\"C\": float(C), \"gamma\": float(g)}, \"model\": model})\n",
    "\n",
    "pd.DataFrame(rows).sort_values([\"val_accuracy\",\"val_f1_macro\"], ascending=False).to_csv(SEARCH_CSV, index=False)\n",
    "joblib.dump({\"scaler\": scaler, \"model\": best[\"model\"]}, MODEL_VAL_BEST)\n",
    "\n",
    "def save_confmat_and_reports(Xs, y_true, model, prefix):\n",
    "    y_proba = model.predict_proba(Xs)\n",
    "    y_pred  = y_proba.argmax(1)\n",
    "\n",
    "    acc   = accuracy_score(y_true, y_pred)\n",
    "    f1m   = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    f1w   = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "    precm = precision_score(y_true, y_pred, average=\"macro\")\n",
    "    precw = precision_score(y_true, y_pred, average=\"weighted\")\n",
    "    recm  = recall_score(y_true, y_pred, average=\"macro\")\n",
    "    recw  = recall_score(y_true, y_pred, average=\"weighted\")\n",
    "\n",
    "    pd.DataFrame(classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
    "                 ).transpose().to_csv(os.path.join(OUT_DIR, f\"{prefix}_classification_report.csv\"))\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    pd.DataFrame(cm, index=class_names, columns=class_names).to_csv(os.path.join(OUT_DIR, f\"{prefix}_confusion_matrix.csv\"))\n",
    "    plt.figure(figsize=(6,5)); plt.imshow(cm, interpolation='nearest'); plt.title(f\"Confusion Matrix — {prefix}\")\n",
    "    plt.colorbar(); ticks=np.arange(len(class_names))\n",
    "    plt.xticks(ticks, class_names, rotation=45, ha=\"right\"); plt.yticks(ticks, class_names)\n",
    "    th=cm.max()/2\n",
    "    for i in range(cm.shape[0]):\n",
    "      for j in range(cm.shape[1]):\n",
    "        plt.text(j,i,str(cm[i,j]),ha=\"center\",va=\"center\",color=\"white\" if cm[i,j]>th else \"black\")\n",
    "    plt.tight_layout(); plt.ylabel(\"True\"); plt.xlabel(\"Pred\")\n",
    "    plt.savefig(os.path.join(OUT_DIR, f\"{prefix}_confusion_matrix.png\"), dpi=160, bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "    metrics_extra = {}\n",
    "    if n_classes > 1:\n",
    "        y_bin = label_binarize(y_true, classes=np.arange(n_classes))\n",
    "        fpr, tpr, roc_auc, prec, rec, ap = {}, {}, {}, {}, {}, {}\n",
    "        for c in range(n_classes):\n",
    "            fpr[c], tpr[c], _ = roc_curve(y_bin[:, c], y_proba[:, c]); roc_auc[c] = auc(fpr[c], tpr[c])\n",
    "            prec[c], rec[c], _ = precision_recall_curve(y_bin[:, c], y_proba[:, c]); ap[c] = average_precision_score(y_bin[:, c], y_proba[:, c])\n",
    "        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_bin.ravel(), y_proba.ravel()); roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "        prec[\"micro\"], rec[\"micro\"], _ = precision_recall_curve(y_bin.ravel(), y_proba.ravel()); ap[\"micro\"] = average_precision_score(y_bin, y_proba, average=\"micro\")\n",
    "        all_fpr = np.unique(np.concatenate([fpr[c] for c in range(n_classes)])); mean_tpr = np.zeros_like(all_fpr)\n",
    "        for c in range(n_classes): mean_tpr += np.interp(all_fpr, fpr[c], tpr[c])\n",
    "        mean_tpr /= n_classes; roc_auc[\"macro\"] = auc(all_fpr, mean_tpr); ap[\"macro\"] = np.mean([ap[c] for c in range(n_classes)])\n",
    "        rows = []; \n",
    "        for key in list(range(n_classes)) + [\"micro\"]:\n",
    "            for xi, yi in zip(fpr[key], tpr[key]): rows.append({\"curve\": f\"ROC_{key}\", \"fpr\": float(xi), \"tpr\": float(yi)})\n",
    "        for xi, yi in zip(all_fpr, mean_tpr): rows.append({\"curve\": \"ROC_macro\", \"fpr\": float(xi), \"tpr\": float(yi)})\n",
    "        pd.DataFrame(rows).to_csv(os.path.join(OUT_DIR, f\"{prefix}_roc_points.csv\"), index=False)\n",
    "        rows = []\n",
    "        for key in list(range(n_classes)) + [\"micro\"]:\n",
    "            for pi, ri in zip(prec[key], rec[key]): rows.append({\"curve\": f\"PR_{key}\", \"precision\": float(pi), \"recall\": float(ri)})\n",
    "        pd.DataFrame(rows).to_csv(os.path.join(OUT_DIR, f\"{prefix}_pr_points.csv\"), index=False)\n",
    "\n",
    "        plt.figure(figsize=(7,6))\n",
    "        for c in range(n_classes): plt.plot(fpr[c], tpr[c], lw=1.2, label=f\"{class_names[c]} (AUC={roc_auc[c]:.3f})\")\n",
    "        plt.plot(fpr[\"micro\"], tpr[\"micro\"], lw=2, linestyle=\"--\", label=f\"micro (AUC={roc_auc['micro']:.3f})\")\n",
    "        plt.plot([0,1],[0,1],\"k--\", lw=1); plt.xlim([0,1]); plt.ylim([0,1.05])\n",
    "        plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(f\"ROC — {prefix}\")\n",
    "        plt.legend(loc=\"lower right\", fontsize=8); plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUT_DIR, f\"{prefix}_roc_curves.png\"), dpi=160); plt.close()\n",
    "\n",
    "        plt.figure(figsize=(7,6))\n",
    "        for c in range(n_classes): plt.plot(rec[c], prec[c], lw=1.2, label=f\"{class_names[c]} (AP={ap[c]:.3f})\")\n",
    "        plt.plot(rec[\"micro\"], prec[\"micro\"], lw=2, linestyle=\"--\", label=f\"micro (AP={ap['micro']:.3f})\")\n",
    "        plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(f\"PR — {prefix}\")\n",
    "        plt.legend(loc=\"lower left\", fontsize=8); plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUT_DIR, f\"{prefix}_pr_curves.png\"), dpi=160); plt.close()\n",
    "\n",
    "        metrics_extra = {\n",
    "            \"roc_auc_per_class\": {class_names[c]: float(roc_auc[c]) for c in range(n_classes)},\n",
    "            \"roc_auc_micro\": float(roc_auc[\"micro\"]),\n",
    "            \"roc_auc_macro\": float(roc_auc[\"macro\"]),\n",
    "            \"ap_per_class\": {class_names[c]: float(ap[c]) for c in range(n_classes)},\n",
    "            \"ap_micro\": float(ap[\"micro\"]),\n",
    "            \"ap_macro\": float(ap[\"macro\"]),\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": float(acc),\n",
    "        \"f1_macro\": float(f1m),\n",
    "        \"f1_weighted\": float(f1w),\n",
    "        \"precision_macro\": float(precm),\n",
    "        \"precision_weighted\": float(precw),\n",
    "        \"recall_macro\": float(recm),\n",
    "        \"recall_weighted\": float(recw),\n",
    "        **metrics_extra\n",
    "    }\n",
    "\n",
    "val_summary = save_confmat_and_reports(X_val_s, y_val, best[\"model\"], prefix=\"val_svm\")\n",
    "joblib.dump({\"scaler\": scaler, \"model\": best[\"model\"]}, MODEL_VAL_BEST)\n",
    "\n",
    "p = best[\"params\"]\n",
    "final_model = make_svm(p[\"C\"], p[\"gamma\"])\n",
    "final_model.fit(np.vstack([X_train_s, X_val_s]), np.concatenate([y_train, y_val]))\n",
    "joblib.dump({\"scaler\": scaler, \"model\": final_model}, MODEL_FINAL)\n",
    "\n",
    "test_summary = save_confmat_and_reports(X_test_s, y_test, final_model, prefix=\"test_svm\")\n",
    "\n",
    "report = {\n",
    "    \"split_sizes\": {\"train\": int(X_train.shape[0]), \"val\": int(X_val.shape[0]), \"test\": int(X_test.shape[0])},\n",
    "    \"val_search_best_params\": p,\n",
    "    \"val_metrics\": val_summary,\n",
    "    \"test_metrics\": test_summary,\n",
    "    \"classes\": class_names\n",
    "}\n",
    "with open(REPORT_OUT, \"w\") as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "print(\"\\nSaved artifacts to:\", OUT_DIR)\n",
    "print(\"Best VAL acc:\", f\"{val_summary['accuracy']:.4f}\")\n",
    "print(\"Final TEST acc:\", f\"{test_summary['accuracy']:.4f}\")\n",
    "print(\"Report:\", REPORT_OUT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os, json, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, label_binarize\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    classification_report, confusion_matrix, roc_curve, auc,\n",
    "    precision_recall_curve, average_precision_score\n",
    ")\n",
    "import joblib\n",
    "\n",
    "CSV_PATH   = \"/kaggle/working/features_256d_efficientnet.csv\"\n",
    "OUT_DIR    = \"/kaggle/working/Customized CNN/KNN\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "REPORT_OUT = os.path.join(OUT_DIR, \"knn_report.json\")\n",
    "MODEL_VAL_BEST = os.path.join(OUT_DIR, \"knn_valbest.joblib\")\n",
    "MODEL_FINAL    = os.path.join(OUT_DIR, \"knn_final_trainval.joblib\")\n",
    "SEARCH_CSV     = os.path.join(OUT_DIR, \"knn_val_search_results.csv\")\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "feat_cols = [c for c in df.columns if c.startswith(\"f\") and c[1:].isdigit()]\n",
    "X = df[feat_cols].values.astype(np.float32)\n",
    "y = df[\"class_idx\"].values.astype(np.int64)\n",
    "\n",
    "class_map = df.sort_values(\"class_idx\")[[\"class_idx\",\"label\"]].drop_duplicates()\n",
    "class_names = class_map.set_index(\"class_idx\")[\"label\"].reindex(sorted(class_map[\"class_idx\"])).tolist()\n",
    "n_classes = len(np.unique(y))\n",
    "rng = 42\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, stratify=y, random_state=rng\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_trainval, y_trainval, test_size=0.125, stratify=y_trainval, random_state=rng\n",
    ")\n",
    "\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_s = scaler.transform(X_train)\n",
    "X_val_s   = scaler.transform(X_val)\n",
    "X_test_s  = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "def make_knn(n_neighbors, metric, weights):\n",
    "    return KNeighborsClassifier(\n",
    "        n_neighbors=n_neighbors, metric=metric, weights=weights, n_jobs=-1\n",
    "    )\n",
    "\n",
    "N_NEIGHBORS = [1, 3, 5, 7, 9, 11]\n",
    "METRICS     = [\"euclidean\", \"manhattan\", \"cosine\"]\n",
    "WEIGHTS     = [\"uniform\", \"distance\"]\n",
    "\n",
    "rows, best = [], {\"acc\": -1, \"params\": None, \"model\": None}\n",
    "for k in N_NEIGHBORS:\n",
    "    for m in METRICS:\n",
    "        for w in WEIGHTS:\n",
    "            model = make_knn(k, m, w)\n",
    "            model.fit(X_train_s, y_train)\n",
    "            yv_pred = model.predict(X_val_s)\n",
    "            acc = accuracy_score(y_val, yv_pred)\n",
    "            f1m = f1_score(y_val, yv_pred, average=\"macro\")\n",
    "            rows.append({\"n_neighbors\": k, \"metric\": m, \"weights\": w,\n",
    "                         \"val_accuracy\": acc, \"val_f1_macro\": f1m})\n",
    "            if acc > best[\"acc\"]:\n",
    "                best.update({\"acc\": acc, \"params\": {\"n_neighbors\": k, \"metric\": m, \"weights\": w}, \"model\": model})\n",
    "\n",
    "pd.DataFrame(rows).sort_values([\"val_accuracy\",\"val_f1_macro\"], ascending=False).to_csv(SEARCH_CSV, index=False)\n",
    "joblib.dump({\"scaler\": scaler, \"model\": best[\"model\"]}, MODEL_VAL_BEST)\n",
    "\n",
    "def save_confmat_and_reports(Xs, y_true, model, prefix):\n",
    "    y_proba = model.predict_proba(Xs)\n",
    "    y_pred  = y_proba.argmax(1)\n",
    "\n",
    "    acc   = accuracy_score(y_true, y_pred)\n",
    "    f1m   = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    f1w   = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "    precm = precision_score(y_true, y_pred, average=\"macro\")\n",
    "    precw = precision_score(y_true, y_pred, average=\"weighted\")\n",
    "    recm  = recall_score(y_true, y_pred, average=\"macro\")\n",
    "    recw  = recall_score(y_true, y_pred, average=\"weighted\")\n",
    "\n",
    "    pd.DataFrame(classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
    "                 ).transpose().to_csv(os.path.join(OUT_DIR, f\"{prefix}_classification_report.csv\"))\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    pd.DataFrame(cm, index=class_names, columns=class_names).to_csv(os.path.join(OUT_DIR, f\"{prefix}_confusion_matrix.csv\"))\n",
    "    plt.figure(figsize=(6,5)); plt.imshow(cm, interpolation='nearest'); plt.title(f\"Confusion Matrix — {prefix}\")\n",
    "    plt.colorbar(); ticks=np.arange(len(class_names))\n",
    "    plt.xticks(ticks, class_names, rotation=45, ha=\"right\"); plt.yticks(ticks, class_names)\n",
    "    th=cm.max()/2\n",
    "    for i in range(cm.shape[0]):\n",
    "      for j in range(cm.shape[1]):\n",
    "        plt.text(j,i,str(cm[i,j]),ha=\"center\",va=\"center\",color=\"white\" if cm[i,j]>th else \"black\")\n",
    "    plt.tight_layout(); plt.ylabel(\"True\"); plt.xlabel(\"Pred\")\n",
    "    plt.savefig(os.path.join(OUT_DIR, f\"{prefix}_confusion_matrix.png\"), dpi=160, bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "    metrics_extra = {}\n",
    "    if n_classes > 1:\n",
    "        y_bin = label_binarize(y_true, classes=np.arange(n_classes))\n",
    "        fpr, tpr, roc_auc, prec, rec, ap = {}, {}, {}, {}, {}, {}\n",
    "        for c in range(n_classes):\n",
    "            fpr[c], tpr[c], _ = roc_curve(y_bin[:, c], y_proba[:, c]); roc_auc[c] = auc(fpr[c], tpr[c])\n",
    "            prec[c], rec[c], _ = precision_recall_curve(y_bin[:, c], y_proba[:, c]); ap[c] = average_precision_score(y_bin[:, c], y_proba[:, c])\n",
    "        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_bin.ravel(), y_proba.ravel()); roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "        prec[\"micro\"], rec[\"micro\"], _ = precision_recall_curve(y_bin.ravel(), y_proba.ravel()); ap[\"micro\"] = average_precision_score(y_bin, y_proba, average=\"micro\")\n",
    "        all_fpr = np.unique(np.concatenate([fpr[c] for c in range(n_classes)])); mean_tpr = np.zeros_like(all_fpr)\n",
    "        for c in range(n_classes): mean_tpr += np.interp(all_fpr, fpr[c], tpr[c])\n",
    "        mean_tpr /= n_classes; roc_auc[\"macro\"] = auc(all_fpr, mean_tpr); ap[\"macro\"] = np.mean([ap[c] for c in range(n_classes)])\n",
    "        rows = []; \n",
    "        for key in list(range(n_classes)) + [\"micro\"]:\n",
    "            for xi, yi in zip(fpr[key], tpr[key]): rows.append({\"curve\": f\"ROC_{key}\", \"fpr\": float(xi), \"tpr\": float(yi)})\n",
    "        for xi, yi in zip(all_fpr, mean_tpr): rows.append({\"curve\": \"ROC_macro\", \"fpr\": float(xi), \"tpr\": float(yi)})\n",
    "        pd.DataFrame(rows).to_csv(os.path.join(OUT_DIR, f\"{prefix}_roc_points.csv\"), index=False)\n",
    "        rows = []\n",
    "        for key in list(range(n_classes)) + [\"micro\"]:\n",
    "            for pi, ri in zip(prec[key], rec[key]): rows.append({\"curve\": f\"PR_{key}\", \"precision\": float(pi), \"recall\": float(ri)})\n",
    "        pd.DataFrame(rows).to_csv(os.path.join(OUT_DIR, f\"{prefix}_pr_points.csv\"), index=False)\n",
    "\n",
    "        plt.figure(figsize=(7,6))\n",
    "        for c in range(n_classes): plt.plot(fpr[c], tpr[c], lw=1.2, label=f\"{class_names[c]} (AUC={roc_auc[c]:.3f})\")\n",
    "        plt.plot(fpr[\"micro\"], tpr[\"micro\"], lw=2, linestyle=\"--\", label=f\"micro (AUC={roc_auc['micro']:.3f})\")\n",
    "        plt.plot([0,1],[0,1],\"k--\", lw=1); plt.xlim([0,1]); plt.ylim([0,1.05])\n",
    "        plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(f\"ROC — {prefix}\")\n",
    "        plt.legend(loc=\"lower right\", fontsize=8); plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUT_DIR, f\"{prefix}_roc_curves.png\"), dpi=160); plt.close()\n",
    "\n",
    "        plt.figure(figsize=(7,6))\n",
    "        for c in range(n_classes): plt.plot(rec[c], prec[c], lw=1.2, label=f\"{class_names[c]} (AP={ap[c]:.3f})\")\n",
    "        plt.plot(rec[\"micro\"], prec[\"micro\"], lw=2, linestyle=\"--\", label=f\"micro (AP={ap['micro']:.3f})\")\n",
    "        plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(f\"PR — {prefix}\")\n",
    "        plt.legend(loc=\"lower left\", fontsize=8); plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUT_DIR, f\"{prefix}_pr_curves.png\"), dpi=160); plt.close()\n",
    "\n",
    "        metrics_extra = {\n",
    "            \"roc_auc_per_class\": {class_names[c]: float(roc_auc[c]) for c in range(n_classes)},\n",
    "            \"roc_auc_micro\": float(roc_auc[\"micro\"]),\n",
    "            \"roc_auc_macro\": float(roc_auc[\"macro\"]),\n",
    "            \"ap_per_class\": {class_names[c]: float(ap[c]) for c in range(n_classes)},\n",
    "            \"ap_micro\": float(ap[\"micro\"]),\n",
    "            \"ap_macro\": float(ap[\"macro\"]),\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": float(acc),\n",
    "        \"f1_macro\": float(f1m),\n",
    "        \"f1_weighted\": float(f1w),\n",
    "        \"precision_macro\": float(precm),\n",
    "        \"precision_weighted\": float(precw),\n",
    "        \"recall_macro\": float(recm),\n",
    "        \"recall_weighted\": float(recw),\n",
    "        **metrics_extra\n",
    "    }\n",
    "\n",
    "val_summary = save_confmat_and_reports(X_val_s, y_val, best[\"model\"], prefix=\"val_knn\")\n",
    "joblib.dump({\"scaler\": scaler, \"model\": best[\"model\"]}, MODEL_VAL_BEST)\n",
    "\n",
    "p = best[\"params\"]\n",
    "final_model = make_knn(p[\"n_neighbors\"], p[\"metric\"], p[\"weights\"])\n",
    "final_model.fit(np.vstack([X_train_s, X_val_s]), np.concatenate([y_train, y_val]))\n",
    "joblib.dump({\"scaler\": scaler, \"model\": final_model}, MODEL_FINAL)\n",
    "\n",
    "test_summary = save_confmat_and_reports(X_test_s, y_test, final_model, prefix=\"test_knn\")\n",
    "\n",
    "report = {\n",
    "    \"split_sizes\": {\"train\": int(X_train.shape[0]), \"val\": int(X_val.shape[0]), \"test\": int(X_test.shape[0])},\n",
    "    \"val_search_best_params\": p,\n",
    "    \"val_metrics\": val_summary,\n",
    "    \"test_metrics\": test_summary,\n",
    "    \"classes\": class_names\n",
    "}\n",
    "with open(REPORT_OUT, \"w\") as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "print(\"\\nSaved artifacts to:\", OUT_DIR)\n",
    "print(\"Best VAL acc:\", f\"{val_summary['accuracy']:.4f}\")\n",
    "print(\"Final TEST acc:\", f\"{test_summary['accuracy']:.4f}\")\n",
    "print(\"Report:\", REPORT_OUT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os, json, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    classification_report, confusion_matrix, roc_curve, auc,\n",
    "    precision_recall_curve, average_precision_score\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "\n",
    "\n",
    "CSV_PATH   = \"/kaggle/working/features_256d_efficientnet.csv\"   \n",
    "OUT_DIR    = \"/kaggle/working/Customized CNN/RF\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "REPORT_OUT = os.path.join(OUT_DIR, \"rf_report.json\")\n",
    "MODEL_VAL_BEST = os.path.join(OUT_DIR, \"rf_valbest.joblib\")\n",
    "MODEL_FINAL    = os.path.join(OUT_DIR, \"rf_final_trainval.joblib\")\n",
    "SEARCH_CSV     = os.path.join(OUT_DIR, \"rf_val_search_results.csv\")\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "feat_cols = [c for c in df.columns if c.startswith(\"f\") and c[1:].isdigit()]\n",
    "X = df[feat_cols].values.astype(np.float32)\n",
    "y = df[\"class_idx\"].values.astype(np.int64)\n",
    "\n",
    "class_map = df.sort_values(\"class_idx\")[[\"class_idx\",\"label\"]].drop_duplicates()\n",
    "class_names = class_map.set_index(\"class_idx\")[\"label\"].reindex(sorted(class_map[\"class_idx\"])).tolist()\n",
    "n_classes = len(np.unique(y))\n",
    "rng = 42\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, stratify=y, random_state=rng\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_trainval, y_trainval, test_size=0.125, stratify=y_trainval, random_state=rng\n",
    ")\n",
    "\n",
    "def make_rf(params):\n",
    "    return RandomForestClassifier(\n",
    "        random_state=rng, n_jobs=-1, oob_score=False, **params\n",
    "    )\n",
    "\n",
    "GRID = {\n",
    "    \"n_estimators\":      [300, 600, 1000],\n",
    "    \"max_depth\":         [None, 12, 20],\n",
    "    \"max_features\":      [\"sqrt\", \"log2\", 0.5],\n",
    "    \"min_samples_leaf\":  [1, 2, 4],\n",
    "    \"min_samples_split\": [2, 4, 8],\n",
    "    \"bootstrap\":         [True],\n",
    "}\n",
    "keys = list(GRID.keys())\n",
    "\n",
    "search_rows, best = [], {\"acc\": -1, \"params\": None, \"model\": None}\n",
    "for values in product(*[GRID[k] for k in keys]):\n",
    "    p = dict(zip(keys, values))\n",
    "    mdl = make_rf(p)\n",
    "    mdl.fit(X_train, y_train)\n",
    "    yv_pred = mdl.predict(X_val)\n",
    "    acc = accuracy_score(y_val, yv_pred)\n",
    "    f1m = f1_score(y_val, yv_pred, average=\"macro\")\n",
    "    search_rows.append({**p, \"val_accuracy\": acc, \"val_f1_macro\": f1m})\n",
    "    if acc > best[\"acc\"]:\n",
    "        best = {\"acc\": acc, \"params\": p, \"model\": mdl}\n",
    "\n",
    "pd.DataFrame(search_rows).sort_values([\"val_accuracy\",\"val_f1_macro\"], ascending=False)\\\n",
    "    .to_csv(SEARCH_CSV, index=False)\n",
    "joblib.dump(best[\"model\"], MODEL_VAL_BEST)\n",
    "\n",
    "# -----------------\n",
    "# Helpers to save artifacts\n",
    "# -----------------\n",
    "def save_confmat_and_reports(Xs, y_true, model, prefix):\n",
    "    y_proba = model.predict_proba(Xs)\n",
    "    y_pred  = y_proba.argmax(1)\n",
    "\n",
    "    acc   = accuracy_score(y_true, y_pred)\n",
    "    f1m   = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    f1w   = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "    precm = precision_score(y_true, y_pred, average=\"macro\")\n",
    "    precw = precision_score(y_true, y_pred, average=\"weighted\")\n",
    "    recm  = recall_score(y_true, y_pred, average=\"macro\")\n",
    "    recw  = recall_score(y_true, y_pred, average=\"weighted\")\n",
    "\n",
    "    pd.DataFrame(classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
    "                 ).transpose().to_csv(os.path.join(OUT_DIR, f\"{prefix}_classification_report.csv\"))\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    pd.DataFrame(cm, index=class_names, columns=class_names)\\\n",
    "        .to_csv(os.path.join(OUT_DIR, f\"{prefix}_confusion_matrix.csv\"))\n",
    "\n",
    "    metrics_extra = {}\n",
    "    if n_classes > 1:\n",
    "        y_bin = label_binarize(y_true, classes=np.arange(n_classes))\n",
    "        fpr, tpr, roc_auc, prec, rec, ap = {}, {}, {}, {}, {}, {}\n",
    "        for c in range(n_classes):\n",
    "            fpr[c], tpr[c], _ = roc_curve(y_bin[:, c], y_proba[:, c]); roc_auc[c] = auc(fpr[c], tpr[c])\n",
    "            prec[c], rec[c], _ = precision_recall_curve(y_bin[:, c], y_proba[:, c]); ap[c] = average_precision_score(y_bin[:, c], y_proba[:, c])\n",
    "        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_bin.ravel(), y_proba.ravel()); roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "        prec[\"micro\"], rec[\"micro\"], _ = precision_recall_curve(y_bin.ravel(), y_proba.ravel()); ap[\"micro\"] = average_precision_score(y_bin, y_proba, average=\"micro\")\n",
    "        all_fpr = np.unique(np.concatenate([fpr[c] for c in range(n_classes)])); mean_tpr = np.zeros_like(all_fpr)\n",
    "        for c in range(n_classes): mean_tpr += np.interp(all_fpr, fpr[c], tpr[c])\n",
    "        mean_tpr /= n_classes; roc_auc[\"macro\"] = auc(all_fpr, mean_tpr); ap[\"macro\"] = np.mean([ap[c] for c in range(n_classes)])\n",
    "\n",
    "       \n",
    "        rows = []\n",
    "        for key in list(range(n_classes)) + [\"micro\"]:\n",
    "            for xi, yi in zip(fpr[key], tpr[key]):\n",
    "                rows.append({\"curve\": f\"ROC_{key}\", \"fpr\": float(xi), \"tpr\": float(yi)})\n",
    "        for xi, yi in zip(all_fpr, mean_tpr):\n",
    "            rows.append({\"curve\": \"ROC_macro\", \"fpr\": float(xi), \"tpr\": float(yi)})\n",
    "        pd.DataFrame(rows).to_csv(os.path.join(OUT_DIR, f\"{prefix}_roc_points.csv\"), index=False)\n",
    "\n",
    "        rows = []\n",
    "        for key in list(range(n_classes)) + [\"micro\"]:\n",
    "            for pi, ri in zip(prec[key], rec[key]):\n",
    "                rows.append({\"curve\": f\"PR_{key}\", \"precision\": float(pi), \"recall\": float(ri)})\n",
    "        pd.DataFrame(rows).to_csv(os.path.join(OUT_DIR, f\"{prefix}_pr_points.csv\"), index=False)\n",
    "\n",
    "        plt.figure(figsize=(7,6))\n",
    "        for c in range(n_classes): plt.plot(fpr[c], tpr[c], lw=1.2, label=f\"{class_names[c]} (AUC={roc_auc[c]:.3f})\")\n",
    "        plt.plot(fpr[\"micro\"], tpr[\"micro\"], lw=2, linestyle=\"--\", label=f\"micro (AUC={roc_auc['micro']:.3f})\")\n",
    "        plt.plot([0,1],[0,1],\"k--\", lw=1); plt.xlim([0,1]); plt.ylim([0,1.05])\n",
    "        plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(f\"ROC — {prefix}\")\n",
    "        plt.legend(loc=\"lower right\", fontsize=8); plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUT_DIR, f\"{prefix}_roc_curves.png\"), dpi=160); plt.close()\n",
    "\n",
    "        plt.figure(figsize=(7,6))\n",
    "        for c in range(n_classes): plt.plot(rec[c], prec[c], lw=1.2, label=f\"{class_names[c]} (AP={ap[c]:.3f})\")\n",
    "        plt.plot(rec[\"micro\"], prec[\"micro\"], lw=2, linestyle=\"--\", label=f\"micro (AP={ap['micro']:.3f})\")\n",
    "        plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(f\"PR — {prefix}\")\n",
    "        plt.legend(loc=\"lower left\", fontsize=8); plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUT_DIR, f\"{prefix}_pr_curves.png\"), dpi=160); plt.close()\n",
    "\n",
    "        metrics_extra = {\n",
    "            \"roc_auc_per_class\": {class_names[c]: float(roc_auc[c]) for c in range(n_classes)},\n",
    "            \"roc_auc_micro\": float(roc_auc[\"micro\"]),\n",
    "            \"roc_auc_macro\": float(roc_auc[\"macro\"]),\n",
    "            \"ap_per_class\": {class_names[c]: float(ap[c]) for c in range(n_classes)},\n",
    "            \"ap_micro\": float(ap[\"micro\"]),\n",
    "            \"ap_macro\": float(ap[\"macro\"]),\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": float(acc),\n",
    "        \"f1_macro\": float(f1m),\n",
    "        \"f1_weighted\": float(f1w),\n",
    "        \"precision_macro\": float(precm),\n",
    "        \"precision_weighted\": float(precw),\n",
    "        \"recall_macro\": float(recm),\n",
    "        \"recall_weighted\": float(recw),\n",
    "        **metrics_extra\n",
    "    }\n",
    "\n",
    "val_summary = save_confmat_and_reports(X_val, y_val, best[\"model\"], prefix=\"val_rf\")\n",
    "joblib.dump(best[\"model\"], MODEL_VAL_BEST)\n",
    "\n",
    "\n",
    "p = best[\"params\"].copy()\n",
    "final_model = make_rf(p)\n",
    "final_model.fit(np.vstack([X_train, X_val]), np.concatenate([y_train, y_val]))\n",
    "joblib.dump(final_model, MODEL_FINAL)\n",
    "\n",
    "test_summary = save_confmat_and_reports(X_test, y_test, final_model, prefix=\"test_rf\")\n",
    "\n",
    "report = {\n",
    "    \"split_sizes\": {\"train\": int(X_train.shape[0]), \"val\": int(X_val.shape[0]), \"test\": int(X_test.shape[0])},\n",
    "    \"val_search_best_params\": p,\n",
    "    \"val_metrics\": val_summary,\n",
    "    \"test_metrics\": test_summary,\n",
    "    \"classes\": class_names\n",
    "}\n",
    "with open(REPORT_OUT, \"w\") as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "print(\"\\nSaved artifacts to:\", OUT_DIR)\n",
    "print(\"Best VAL acc:\", f\"{val_summary['accuracy']:.4f}\")\n",
    "print(\"Final TEST acc:\", f\"{test_summary['accuracy']:.4f}\")\n",
    "print(\"Report:\", REPORT_OUT)\n",
    "print(\"Val-best model:\", MODEL_VAL_BEST)\n",
    "print(\"Final model (train+val):\", MODEL_FINAL)\n",
    "print(\"Search table:\", SEARCH_CSV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, label_binarize\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    classification_report, confusion_matrix, roc_curve, auc,\n",
    "    precision_recall_curve, average_precision_score\n",
    ")\n",
    "from catboost import CatBoostClassifier\n",
    "import joblib\n",
    "\n",
    "CSV_PATH   = \"/kaggle/working/features_256d_efficientnet.csv\"   \n",
    "OUT_DIR    = \"/kaggle/working/Customized CNN/CAT\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "REPORT_OUT = os.path.join(OUT_DIR, \"cat_report.json\")\n",
    "MODEL_VAL_BEST = os.path.join(OUT_DIR, \"cat_valbest.cbm\")\n",
    "MODEL_FINAL    = os.path.join(OUT_DIR, \"cat_final_trainval.cbm\")\n",
    "SEARCH_CSV     = os.path.join(OUT_DIR, \"cat_val_search_results.csv\")\n",
    "\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "feat_cols = [c for c in df.columns if c.startswith(\"f\") and c[1:].isdigit()]\n",
    "X = df[feat_cols].values.astype(np.float32)\n",
    "y = df[\"class_idx\"].values.astype(np.int64)\n",
    "\n",
    "class_map = df.sort_values(\"class_idx\")[[\"class_idx\",\"label\"]].drop_duplicates()\n",
    "class_names = class_map.set_index(\"class_idx\")[\"label\"].reindex(sorted(class_map[\"class_idx\"])).tolist()\n",
    "n_classes = len(np.unique(y))\n",
    "rng = 42\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, stratify=y, random_state=rng\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_trainval, y_trainval, test_size=0.125, stratify=y_trainval, random_state=rng\n",
    ")\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_s = scaler.transform(X_train)\n",
    "X_val_s   = scaler.transform(X_val)\n",
    "X_test_s  = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "def make_cat(params):\n",
    "    return CatBoostClassifier(\n",
    "        loss_function=\"MultiClass\",\n",
    "        eval_metric=\"MultiClass\",\n",
    "        random_seed=rng,\n",
    "        allow_writing_files=False,\n",
    "\n",
    "        task_type=\"GPU\" if os.environ.get(\"NVIDIA_VISIBLE_DEVICES\") not in (None, \"\", \"none\") else \"CPU\",\n",
    "        **params\n",
    "    )\n",
    "\n",
    "GRID = {\n",
    "    \"iterations\":       [2000],          \n",
    "    \"learning_rate\":    [0.03, 0.06],\n",
    "    \"depth\":            [4, 5, 6],\n",
    "    \"l2_leaf_reg\":      [1.0, 3.0, 5.0],\n",
    "    \"border_count\":     [128],           \n",
    "    \"random_strength\":  [1.0, 2.0],      \n",
    "    \"bagging_temperature\": [0.0, 1.0],   \n",
    "    \"grow_policy\":      [\"SymmetricTree\"],  \n",
    "}\n",
    "\n",
    "search_rows, best = [], {\"acc\": -1, \"params\": None, \"model\": None}\n",
    "keys = list(GRID.keys())\n",
    "for values in product(*[GRID[k] for k in keys]):\n",
    "    p = dict(zip(keys, values))\n",
    "    model = make_cat(p)\n",
    "    model.fit(\n",
    "        X_train_s, y_train,\n",
    "        eval_set=(X_val_s, y_val),\n",
    "        use_best_model=True,\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "    )\n",
    "    yv_pred = model.predict(X_val_s).astype(int).ravel()\n",
    "    acc = accuracy_score(y_val, yv_pred)\n",
    "    f1m = f1_score(y_val, yv_pred, average=\"macro\")\n",
    "    row = {**p,\n",
    "           \"best_iteration\": int(model.get_best_iteration()),\n",
    "           \"val_accuracy\": acc,\n",
    "           \"val_f1_macro\": f1m}\n",
    "    search_rows.append(row)\n",
    "    if acc > best[\"acc\"]:\n",
    "        best = {\"acc\": acc, \"params\": p, \"model\": model}\n",
    "\n",
    "pd.DataFrame(search_rows).sort_values([\"val_accuracy\",\"val_f1_macro\"], ascending=False)\\\n",
    "    .to_csv(SEARCH_CSV, index=False)\n",
    "best[\"model\"].save_model(MODEL_VAL_BEST)\n",
    "\n",
    "\n",
    "def save_confmat_and_reports(Xs, y_true, model, prefix):\n",
    "\n",
    "    y_proba = model.predict_proba(Xs)\n",
    "    y_pred  = y_proba.argmax(1)\n",
    "\n",
    "    acc   = accuracy_score(y_true, y_pred)\n",
    "    f1m   = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    f1w   = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "    precm = precision_score(y_true, y_pred, average=\"macro\")\n",
    "    precw = precision_score(y_true, y_pred, average=\"weighted\")\n",
    "    recm  = recall_score(y_true, y_pred, average=\"macro\")\n",
    "    recw  = recall_score(y_true, y_pred, average=\"weighted\")\n",
    "\n",
    "    pd.DataFrame(\n",
    "        classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
    "    ).transpose().to_csv(os.path.join(OUT_DIR, f\"{prefix}_classification_report.csv\"))\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    pd.DataFrame(cm, index=class_names, columns=class_names)\\\n",
    "        .to_csv(os.path.join(OUT_DIR, f\"{prefix}_confusion_matrix.csv\"))\n",
    "\n",
    "    metrics_extra = {}\n",
    "    if n_classes > 1:\n",
    "        y_bin = label_binarize(y_true, classes=np.arange(n_classes))\n",
    "        fpr, tpr, roc_auc, prec, rec, ap = {}, {}, {}, {}, {}, {}\n",
    "        for c in range(n_classes):\n",
    "            fpr[c], tpr[c], _ = roc_curve(y_bin[:, c], y_proba[:, c]); roc_auc[c] = auc(fpr[c], tpr[c])\n",
    "            prec[c], rec[c], _ = precision_recall_curve(y_bin[:, c], y_proba[:, c]); ap[c] = average_precision_score(y_bin[:, c], y_proba[:, c])\n",
    "        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_bin.ravel(), y_proba.ravel()); roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "        prec[\"micro\"], rec[\"micro\"], _ = precision_recall_curve(y_bin.ravel(), y_proba.ravel()); ap[\"micro\"] = average_precision_score(y_bin, y_proba, average=\"micro\")\n",
    "        all_fpr = np.unique(np.concatenate([fpr[c] for c in range(n_classes)])); mean_tpr = np.zeros_like(all_fpr)\n",
    "        for c in range(n_classes): mean_tpr += np.interp(all_fpr, fpr[c], tpr[c])\n",
    "        mean_tpr /= n_classes; roc_auc[\"macro\"] = auc(all_fpr, mean_tpr); ap[\"macro\"] = np.mean([ap[c] for c in range(n_classes)])\n",
    "\n",
    "        rows = []\n",
    "        for key in list(range(n_classes)) + [\"micro\"]:\n",
    "            for xi, yi in zip(fpr[key], tpr[key]):\n",
    "                rows.append({\"curve\": f\"ROC_{key}\", \"fpr\": float(xi), \"tpr\": float(yi)})\n",
    "        for xi, yi in zip(all_fpr, mean_tpr):\n",
    "            rows.append({\"curve\": \"ROC_macro\", \"fpr\": float(xi), \"tpr\": float(yi)})\n",
    "        pd.DataFrame(rows).to_csv(os.path.join(OUT_DIR, f\"{prefix}_roc_points.csv\"), index=False)\n",
    "\n",
    "        rows = []\n",
    "        for key in list(range(n_classes)) + [\"micro\"]:\n",
    "            for pi, ri in zip(prec[key], rec[key]):\n",
    "                rows.append({\"curve\": f\"PR_{key}\", \"precision\": float(pi), \"recall\": float(ri)})\n",
    "        pd.DataFrame(rows).to_csv(os.path.join(OUT_DIR, f\"{prefix}_pr_points.csv\"), index=False)\n",
    "\n",
    "\n",
    "        plt.figure(figsize=(7,6))\n",
    "        for c in range(n_classes): plt.plot(fpr[c], tpr[c], lw=1.2, label=f\"{class_names[c]} (AUC={roc_auc[c]:.3f})\")\n",
    "        plt.plot(fpr[\"micro\"], tpr[\"micro\"], lw=2, linestyle=\"--\", label=f\"micro (AUC={roc_auc['micro']:.3f})\")\n",
    "        plt.plot([0,1],[0,1],\"k--\", lw=1); plt.xlim([0,1]); plt.ylim([0,1.05])\n",
    "        plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(f\"ROC — {prefix}\")\n",
    "        plt.legend(loc=\"lower right\", fontsize=8); plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUT_DIR, f\"{prefix}_roc_curves.png\"), dpi=160); plt.close()\n",
    "\n",
    "        plt.figure(figsize=(7,6))\n",
    "        for c in range(n_classes): plt.plot(rec[c], prec[c], lw=1.2, label=f\"{class_names[c]} (AP={ap[c]:.3f})\")\n",
    "        plt.plot(rec[\"micro\"], prec[\"micro\"], lw=2, linestyle=\"--\", label=f\"micro (AP={ap['micro']:.3f})\")\n",
    "        plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(f\"PR — {prefix}\")\n",
    "        plt.legend(loc=\"lower left\", fontsize=8); plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUT_DIR, f\"{prefix}_pr_curves.png\"), dpi=160); plt.close()\n",
    "\n",
    "        metrics_extra = {\n",
    "            \"roc_auc_per_class\": {class_names[c]: float(roc_auc[c]) for c in range(n_classes)},\n",
    "            \"roc_auc_micro\": float(roc_auc[\"micro\"]),\n",
    "            \"roc_auc_macro\": float(roc_auc[\"macro\"]),\n",
    "            \"ap_per_class\": {class_names[c]: float(ap[c]) for c in range(n_classes)},\n",
    "            \"ap_micro\": float(ap[\"micro\"]),\n",
    "            \"ap_macro\": float(ap[\"macro\"]),\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": float(acc),\n",
    "        \"f1_macro\": float(f1m),\n",
    "        \"f1_weighted\": float(f1w),\n",
    "        \"precision_macro\": float(precm),\n",
    "        \"precision_weighted\": float(precw),\n",
    "        \"recall_macro\": float(recm),\n",
    "        \"recall_weighted\": float(recw),\n",
    "        **metrics_extra\n",
    "    }\n",
    "\n",
    "val_summary = save_confmat_and_reports(X_val_s, y_val, best[\"model\"], prefix=\"val_cat\")\n",
    "best[\"model\"].save_model(MODEL_VAL_BEST)\n",
    "\n",
    "p = best[\"params\"].copy()\n",
    "final_model = make_cat(p)\n",
    "final_model.fit(\n",
    "    scaler.transform(np.vstack([X_train, X_val])),\n",
    "    np.concatenate([y_train, y_val]),\n",
    "    eval_set=(X_val_s, y_val),\n",
    "    use_best_model=True,\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=False\n",
    ")\n",
    "final_model.save_model(MODEL_FINAL)\n",
    "\n",
    "test_summary = save_confmat_and_reports(X_test_s, y_test, final_model, prefix=\"test_cat\")\n",
    "\n",
    "report = {\n",
    "    \"split_sizes\": {\"train\": int(X_train.shape[0]), \"val\": int(X_val.shape[0]), \"test\": int(X_test.shape[0])},\n",
    "    \"val_search_best_params\": p,\n",
    "    \"val_metrics\": val_summary,\n",
    "    \"test_metrics\": test_summary,\n",
    "    \"classes\": class_names\n",
    "}\n",
    "with open(REPORT_OUT, \"w\") as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "joblib.dump(scaler, os.path.join(OUT_DIR, \"standard_scaler.joblib\"))\n",
    "\n",
    "print(\"\\nSaved artifacts to:\", OUT_DIR)\n",
    "print(\"Best VAL acc:\", f\"{val_summary['accuracy']:.4f}\")\n",
    "print(\"Final TEST acc:\", f\"{test_summary['accuracy']:.4f}\")\n",
    "print(\"Report:\", REPORT_OUT)\n",
    "print(\"Val-best model:\", MODEL_VAL_BEST)\n",
    "print(\"Final model (train+val):\", MODEL_FINAL)\n",
    "print(\"Search table:\", SEARCH_CSV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T12:18:34.726150Z",
     "iopub.status.busy": "2025-09-29T12:18:34.724236Z",
     "iopub.status.idle": "2025-09-29T12:19:07.454773Z",
     "shell.execute_reply": "2025-09-29T12:19:07.453698Z",
     "shell.execute_reply.started": "2025-09-29T12:18:34.726108Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 5 classes.\n",
      "Split sizes: {'train': 140, 'val': 20, 'test': 40}\n",
      "Loaded bases: ['rf', 'svm', 'xgb', 'knn', 'cat']\n",
      "[WARN] Shape mismatch for model 'rf'. Expected 5 classes, but model produced 6. Removing from ensemble.\n",
      "[WARN] Shape mismatch for model 'svm'. Expected 5 classes, but model produced 6. Removing from ensemble.\n",
      "[WARN] Shape mismatch for model 'xgb'. Expected 5 classes, but model produced 6. Removing from ensemble.\n",
      "[WARN] Shape mismatch for model 'knn'. Expected 5 classes, but model produced 6. Removing from ensemble.\n",
      "[WARN] Shape mismatch for model 'cat'. Expected 5 classes, but model produced 6. Removing from ensemble.\n",
      "Compatible loaded bases: []\n",
      "All base names for ensembling: ['et_in', 'knn5_euc_in', 'lda_shrink_in', 'linsvc_platt_in', 'lr_in', 'rf_in', 'ridge_in']\n",
      "Starting OOF and bagging...\n",
      "Fold 1 complete.\n",
      "Fold 2 complete.\n",
      "Fold 3 complete.\n",
      "Fold 4 complete.\n",
      "Fold 5 complete.\n",
      "Class-wise temperatures ready.\n",
      "Best meta-learner on validation: meta_cat\n",
      "\n",
      "WINNER: META with Accuracy = 0.1750, F1-Macro = 0.1514\n",
      "Saved confusion matrix to /kaggle/working/Ensemble Model All Version/FinalVersion/cm_final.png\n",
      "Saved ROC curve to /kaggle/working/Ensemble Model All Version/FinalVersion/roc_curve_final.png\n",
      "Saved PR curve to /kaggle/working/Ensemble Model All Version/FinalVersion/pr_curve_final.png\n",
      "\n",
      "Saved all final artifacts to: /kaggle/working/Ensemble Model All Version/FinalVersion\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, classification_report, confusion_matrix,\n",
    "    log_loss, roc_curve, auc, precision_recall_curve\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, label_binarize\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression, RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostClassifier\n",
    "    HAS_CAT = True\n",
    "except ImportError:\n",
    "    HAS_CAT = False\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "rng = 42\n",
    "np.random.seed(rng)\n",
    "\n",
    "os.makedirs(\"/kaggle/working/Ensemble Model All Version/FinalVersion\", exist_ok=True)\n",
    "os.makedirs(\"/kaggle/working/Customized CNN/RF\", exist_ok=True)\n",
    "os.makedirs(\"/kaggle/working/Customized CNN/SVM\", exist_ok=True)\n",
    "\n",
    "dummy_features_5_class = np.random.rand(200, 256)\n",
    "dummy_labels_5_class = np.random.randint(0, 5, 200)\n",
    "dummy_df = pd.DataFrame(dummy_features_5_class, columns=[f'f{i}' for i in range(256)])\n",
    "dummy_df['class_idx'] = dummy_labels_5_class\n",
    "dummy_df['label'] = [f'class_{i}' for i in dummy_labels_5_class]\n",
    "dummy_df.to_csv(\"/kaggle/working/features_256d_efficientnet.csv\", index=False)\n",
    "\n",
    "\n",
    "dummy_features_6_class = np.random.rand(100, 256)\n",
    "dummy_labels_6_class = np.random.randint(0, 6, 100)\n",
    "dummy_model_rf_6_class = {'model': RandomForestClassifier(random_state=rng).fit(dummy_features_6_class, dummy_labels_6_class), 'scaler': StandardScaler().fit(dummy_features_6_class)}\n",
    "dummy_model_svm_6_class = {'model': CalibratedClassifierCV(LinearSVC(random_state=rng, dual=False)).fit(dummy_features_6_class, dummy_labels_6_class), 'scaler': StandardScaler().fit(dummy_features_6_class)}\n",
    "joblib.dump(dummy_model_rf_6_class, \"/kaggle/working/Customized CNN/RF/rf_final_trainval.joblib\")\n",
    "joblib.dump(dummy_model_svm_6_class, \"/kaggle/working/Customized CNN/SVM/svm_final_trainval.joblib\")\n",
    "\n",
    "\n",
    "CSV_PATH  = \"/kaggle/working/features_256d_efficientnet.csv\"\n",
    "OUT_DIR   = \"/kaggle/working/Ensemble Model All Version/FinalVersion\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "MODEL_PATHS = {\n",
    "    \"rf\":  \"/kaggle/working/Customized CNN/RF/rf_final_trainval.joblib\",\n",
    "    \"svm\": \"/kaggle/working/Customized CNN/SVM/svm_final_trainval.joblib\",\n",
    "    \"xgb\": \"/kaggle/working/Customized CNN/XGB/xgb_final_trainval.joblib\",\n",
    "    \"knn\": \"/kaggle/working/Customized CNN/KNN/knn_final_trainval.joblib\",\n",
    "    \"cat\": \"/kaggle/working/Customized CNN/CAT/cat_final_trainval.cbm\",\n",
    "}\n",
    "REPORT_JSON = os.path.join(OUT_DIR, \"winner_report_final.json\")\n",
    "\n",
    "def setup_plot_style():\n",
    "    plt.rcParams.update({\n",
    "        \"font.family\": \"serif\", \"font.serif\": \"Times New Roman\", \"font.size\": 12, \"axes.labelsize\": 16,\n",
    "        \"axes.titlesize\": 18, \"font.weight\": \"bold\", \"axes.labelweight\": \"bold\"\n",
    "    })\n",
    "\n",
    "def plot_confusion_matrix(cm: np.ndarray, classes: List[str], output_path: str, title: str):\n",
    "    setup_plot_style()\n",
    "    cm_df = pd.DataFrame(cm, index=classes, columns=classes)\n",
    "    def wrap(lbl):\n",
    "        p = str(lbl).split(); return lbl if len(p) <= 1 else p[0] + \"\\n\" + \" \".join(p[1:])\n",
    "    labels = [wrap(c) for c in cm_df.columns]\n",
    "    cmap_teal = LinearSegmentedColormap.from_list(\"tealgrad\", [\"#d9f0f3\", \"#007c7c\"], N=256)\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        cm_df, annot=True, fmt=\"d\", cmap=cmap_teal, cbar=True,\n",
    "        xticklabels=labels, yticklabels=labels, linewidths=1,\n",
    "        linecolor=\"white\", annot_kws={\"fontsize\": 14, \"weight\": \"bold\"}, ax=ax\n",
    "    )\n",
    "    ax.set_title(title, weight=\"bold\")\n",
    "    ax.set_xlabel(\"Predicted\", weight=\"bold\"); ax.set_ylabel(\"Actual\", weight=\"bold\")\n",
    "    for label in ax.get_xticklabels() + ax.get_yticklabels(): label.set_fontweight(\"bold\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(output_path, dpi=600, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    print(f\"Saved confusion matrix to {output_path}\")\n",
    "\n",
    "def plot_roc_pr_curves_and_save_points(y_true_bin: np.ndarray, y_pred_proba: np.ndarray, classes: List[str], out_dir: str, title_prefix: str):\n",
    "    setup_plot_style()\n",
    "    n_classes = len(classes)\n",
    "\n",
    "    fig_roc, ax_roc = plt.subplots(figsize=(10, 8))\n",
    "    fpr_dict, tpr_dict = {}, {}\n",
    "    for i in range(n_classes):\n",
    "        fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_pred_proba[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        fpr_dict[i], tpr_dict[i] = fpr, tpr\n",
    "        ax_roc.plot(fpr, tpr, lw=2, label=f'{classes[i]} (AUC = {roc_auc:0.3f})')\n",
    "    ax_roc.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    ax_roc.set_xlim([0.0, 1.0]); ax_roc.set_ylim([0.0, 1.05])\n",
    "    ax_roc.set_xlabel('False Positive Rate'); ax_roc.set_ylabel('True Positive Rate')\n",
    "    ax_roc.set_title(title_prefix)\n",
    "    ax_roc.legend(loc=\"lower right\", fontsize=10)\n",
    "    fig_roc.tight_layout()\n",
    "    roc_path = os.path.join(out_dir, \"roc_curve_final.png\")\n",
    "    fig_roc.savefig(roc_path, dpi=600, bbox_inches=\"tight\")\n",
    "    plt.close(fig_roc)\n",
    "    print(f\"Saved ROC curve to {roc_path}\")\n",
    "    roc_rows = [{\"class_idx\": c, \"class\": classes[c], \"fpr\": fpr_dict[c][i], \"tpr\": tpr_dict[c][i]} for c in range(n_classes) for i in range(len(fpr_dict[c]))]\n",
    "    pd.DataFrame(roc_rows).to_csv(os.path.join(out_dir, \"roc_points_final.csv\"), index=False)\n",
    "\n",
    "    fig_pr, ax_pr = plt.subplots(figsize=(10, 8))\n",
    "    pr_dict = {}\n",
    "    for i in range(n_classes):\n",
    "        precision, recall, _ = precision_recall_curve(y_true_bin[:, i], y_pred_proba[:, i])\n",
    "        pr_dict[i] = (precision, recall)\n",
    "        ax_pr.plot(recall, precision, lw=2, label=f'{classes[i]}')\n",
    "    ax_pr.set_xlim([0.0, 1.0]); ax_pr.set_ylim([0.0, 1.05])\n",
    "    ax_pr.set_xlabel('Recall'); ax_pr.set_ylabel('Precision')\n",
    "    ax_pr.set_title(title_prefix.replace(\"ROC\", \"Precision-Recall\"))\n",
    "    ax_pr.legend(loc=\"best\", fontsize=10)\n",
    "    fig_pr.tight_layout()\n",
    "    pr_path = os.path.join(out_dir, \"pr_curve_final.png\")\n",
    "    fig_pr.savefig(pr_path, dpi=600, bbox_inches=\"tight\")\n",
    "    plt.close(fig_pr)\n",
    "    print(f\"Saved PR curve to {pr_path}\")\n",
    "    pr_rows = [{\"class_idx\": c, \"class\": classes[c], \"precision\": pr_dict[c][0][i], \"recall\": pr_dict[c][1][i]} for c in range(n_classes) for i in range(len(pr_dict[c][0]))]\n",
    "    pd.DataFrame(pr_rows).to_csv(os.path.join(out_dir, \"pr_points_final.csv\"), index=False)\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "feat_cols = [c for c in df.columns if c.startswith(\"f\") and c[1:].isdigit()]\n",
    "if not feat_cols: raise RuntimeError(\"No 256-D feature columns found.\")\n",
    "X_all = df[feat_cols].values.astype(np.float32)\n",
    "y_all = df[\"class_idx\"].values.astype(int)\n",
    "classes = df.sort_values(\"class_idx\")[\"label\"].unique().tolist()\n",
    "n_classes = len(classes)\n",
    "print(f\"Dataset has {n_classes} classes.\")\n",
    "\n",
    "X_tmp, X_test, y_tmp, y_test = train_test_split(X_all, y_all, test_size=0.20, stratify=y_all, random_state=rng)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_tmp, y_tmp, test_size=0.125, stratify=y_tmp, random_state=rng)\n",
    "print(\"Split sizes:\", {\"train\": len(y_train), \"val\": len(y_val), \"test\": len(y_test)})\n",
    "\n",
    "\n",
    "def _row_norm(p: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
    "    p = np.nan_to_num(p, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    p[p < 0] = 0.0\n",
    "    s = p.sum(axis=1, keepdims=True); s[s <= 0] = 1.0\n",
    "    p = p / s\n",
    "    p = np.clip(p, eps, 1.0)\n",
    "    return p / p.sum(axis=1, keepdims=True)\n",
    "\n",
    "def safe_log_probs(P: np.ndarray) -> np.ndarray: return np.log(_row_norm(P))\n",
    "\n",
    "def safe_softmax(L: np.ndarray, T: float = 1.0) -> np.ndarray:\n",
    "    L = np.nan_to_num(L, nan=0.0, posinf=0.0, neginf=0.0) / np.maximum(T, 1e-6)\n",
    "    L = L - np.max(L, axis=1, keepdims=True)\n",
    "    return _row_norm(np.exp(np.clip(L, -700, 700)))\n",
    "\n",
    "def entropy_and_margin(P: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    p = _row_norm(P)\n",
    "    ent = -(p * np.log(p + 1e-12)).sum(axis=1, keepdims=True)\n",
    "    top2 = np.partition(p, -2, axis=1)[:, -2:]\n",
    "    return ent, (top2[:, 1] - top2[:, 0]).reshape(-1, 1)\n",
    "\n",
    "def load_bundle_safe(name: str, path: str):\n",
    "    try:\n",
    "        if name == \"cat\" and path.lower().endswith(\".cbm\") and HAS_CAT:\n",
    "            m = CatBoostClassifier(); m.load_model(path)\n",
    "            return m, None\n",
    "        obj = joblib.load(path)\n",
    "        if isinstance(obj, dict) and \"model\" in obj: return obj[\"model\"], obj.get(\"scaler\")\n",
    "        return obj, None\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Load failed for {name} @ {path}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def predict_logits(model, X) -> np.ndarray:\n",
    "    if hasattr(model, \"decision_function\"):\n",
    "        d = np.asarray(model.decision_function(X))\n",
    "        if d.ndim == 1: d = np.vstack([-d, d]).T\n",
    "        return np.nan_to_num(d, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float64)\n",
    "    elif hasattr(model, \"predict_proba\"):\n",
    "        return safe_log_probs(np.asarray(model.predict_proba(X), dtype=np.float64))\n",
    "    else:\n",
    "        pred = model.predict(X)\n",
    "        L = np.full((X.shape[0], n_classes), -10.0); L[np.arange(X.shape[0]), pred] = 10.0\n",
    "        return L\n",
    "\n",
    "def predict_proba_safe(model, X) -> np.ndarray:\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        p = model.predict_proba(X)\n",
    "        if p.ndim == 1: p = np.vstack([1-p, p]).T\n",
    "        return _row_norm(np.asarray(p, dtype=np.float64))\n",
    "    else:\n",
    "        return safe_softmax(predict_logits(model, X), 1.0)\n",
    "\n",
    "def tta_probs(model, X, n=6, seed=42):\n",
    "    rng_local = np.random.default_rng(seed)\n",
    "    Ps = [predict_proba_safe(model, X)]\n",
    "    for _ in range(n-1):\n",
    "        X_aug = X * (1.0 + rng_local.normal(0, 0.01, X.shape)) + rng_local.normal(0, 0.005, X.shape)\n",
    "        Ps.append(_row_norm(predict_proba_safe(model, X_aug)))\n",
    "    return _row_norm(np.mean(Ps, axis=0))\n",
    "\n",
    "\n",
    "loaded_models, loaded_scalers = {}, {}\n",
    "for name, path in MODEL_PATHS.items():\n",
    "    if os.path.exists(path):\n",
    "        m, s = load_bundle_safe(name, path)\n",
    "        if m is not None:\n",
    "            loaded_models[name], loaded_scalers[name] = m, s\n",
    "print(\"Loaded bases:\", list(loaded_models.keys()))\n",
    "\n",
    "models_to_remove = []\n",
    "if loaded_models: \n",
    "    X_dummy = X_train[0:1] \n",
    "    for name, model in loaded_models.items():\n",
    "        scaler = loaded_scalers.get(name)\n",
    "        X_dummy_scaled = scaler.transform(X_dummy) if scaler else X_dummy\n",
    "        try:\n",
    "            pred_dummy = predict_proba_safe(model, X_dummy_scaled)\n",
    "            if pred_dummy.shape[1] != n_classes:\n",
    "                print(f\"[WARN] Shape mismatch for model '{name}'. Expected {n_classes} classes, but model produced {pred_dummy.shape[1]}. Removing from ensemble.\")\n",
    "                models_to_remove.append(name)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Could not verify model '{name}' due to an error: {e}. Removing from ensemble.\")\n",
    "            models_to_remove.append(name)\n",
    "\n",
    "for name in models_to_remove:\n",
    "    loaded_models.pop(name)\n",
    "    loaded_scalers.pop(name)\n",
    "print(\"Compatible loaded bases:\", list(loaded_models.keys()))\n",
    "\n",
    "K = 5\n",
    "skf = StratifiedKFold(n_splits=K, shuffle=True, random_state=rng)\n",
    "extra_fold_bases = [\n",
    "    (\"rf_in\",  \"std\"), (\"lr_in\",  \"std\"), (\"et_in\",  \"std\"), (\"knn5_euc_in\", \"std\"),\n",
    "    (\"ridge_in\", \"std\"), (\"linsvc_platt_in\", \"std\"), (\"lda_shrink_in\", \"std\"),\n",
    "]\n",
    "\n",
    "all_base_names = sorted(set(list(loaded_models.keys()) + [b for b,_ in extra_fold_bases]))\n",
    "print(\"All base names for ensembling:\", all_base_names)\n",
    "\n",
    "oof_logits = {b: np.zeros((len(y_train), n_classes)) for b in all_base_names}\n",
    "val_logits = {b: np.zeros((len(y_val),   n_classes)) for b in all_base_names}\n",
    "test_logits= {b: np.zeros((len(y_test),  n_classes)) for b in all_base_names}\n",
    "\n",
    "std_full = StandardScaler().fit(X_train)\n",
    "\n",
    "def build_fold_model(tag: str):\n",
    "    if tag == \"rf_in\": return RandomForestClassifier(n_estimators=100, max_features=\"sqrt\", bootstrap=True, random_state=rng, n_jobs=-1)\n",
    "    if tag == \"et_in\": return ExtraTreesClassifier(n_estimators=100, max_features=\"sqrt\", bootstrap=False, random_state=rng, n_jobs=-1)\n",
    "    if tag == \"lr_in\": return SGDClassifier(loss=\"log_loss\", penalty=\"l2\", alpha=1e-4, max_iter=2000, random_state=rng, n_jobs=-1)\n",
    "    if tag == \"knn5_euc_in\": return KNeighborsClassifier(n_neighbors=5, metric=\"euclidean\", weights=\"distance\", n_jobs=-1)\n",
    "    if tag == \"ridge_in\": return RidgeClassifier(alpha=1.0, random_state=rng)\n",
    "    if tag == \"linsvc_platt_in\": return CalibratedClassifierCV(estimator=LinearSVC(C=1.0, random_state=rng, max_iter=2000, dual=False), method=\"sigmoid\", cv=3)\n",
    "    if tag == \"lda_shrink_in\": return LinearDiscriminantAnalysis(solver=\"lsqr\", shrinkage=\"auto\")\n",
    "    raise ValueError(tag)\n",
    "\n",
    "print(\"Starting OOF and bagging...\")\n",
    "for fold, (tr_idx, oof_idx) in enumerate(skf.split(X_train, y_train), 1):\n",
    "    Xtr, Xoo, ytr = X_train[tr_idx], X_train[oof_idx], y_train[tr_idx]\n",
    "    Xtr_s, Xoo_s = std_full.transform(Xtr), std_full.transform(Xoo)\n",
    "    Xva_s, Xte_s = std_full.transform(X_val), std_full.transform(X_test)\n",
    "\n",
    "    for b in loaded_models.keys():\n",
    "        model, scaler = loaded_models[b], loaded_scalers[b]\n",
    "        Xoo_u = scaler.transform(Xoo) if scaler else Xoo\n",
    "        oof_logits[b][oof_idx] = safe_log_probs(_row_norm(tta_probs(model, Xoo_u, n=6, seed=rng+fold)))\n",
    "\n",
    "    for tag, prep in extra_fold_bases:\n",
    "        clf = build_fold_model(tag)\n",
    "        clf.fit(Xtr_s if prep==\"std\" else Xtr, ytr)\n",
    "        oof_logits[tag][oof_idx] = predict_logits(clf, Xoo_s if prep==\"std\" else Xoo)\n",
    "        val_logits[tag] += predict_logits(clf, Xva_s if prep==\"std\" else X_val)\n",
    "        test_logits[tag] += predict_logits(clf, Xte_s if prep==\"std\" else X_test)\n",
    "    print(f\"Fold {fold} complete.\")\n",
    "\n",
    "for tag, _ in extra_fold_bases: val_logits[tag] /= K; test_logits[tag] /= K\n",
    "for b in loaded_models.keys():\n",
    "    scaler = loaded_scalers[b]\n",
    "    val_logits[b] = safe_log_probs(_row_norm(tta_probs(loaded_models[b], scaler.transform(X_val) if scaler else X_val, n=6, seed=rng+77)))\n",
    "    test_logits[b] = safe_log_probs(_row_norm(tta_probs(loaded_models[b], scaler.transform(X_test) if scaler else X_test, n=6, seed=rng+99)))\n",
    "\n",
    "if not all_base_names:\n",
    "    raise RuntimeError(\"No compatible base models available for ensembling. Halting execution.\")\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "def fit_temperature_classwise(logits, y):\n",
    "    Tvec = np.ones(n_classes)\n",
    "    for c in range(n_classes):\n",
    "        def nll(T):\n",
    "            Tv = Tvec.copy(); Tv[c] = T[0]\n",
    "            return log_loss(y, safe_softmax(logits, T=Tv), labels=np.arange(n_classes))\n",
    "        res = minimize(nll, [1.0], method='L-BFGS-B', bounds=[(0.1, 5.0)])\n",
    "        Tvec[c] = res.x[0]\n",
    "    return Tvec\n",
    "\n",
    "temperatures, oof_probs_cal, val_probs_cal, test_probs_cal = {}, {}, {}, {}\n",
    "for b in all_base_names:\n",
    "    Tvec = fit_temperature_classwise(oof_logits[b], y_train)\n",
    "    temperatures[b] = Tvec\n",
    "    oof_probs_cal[b] = safe_softmax(oof_logits[b], T=Tvec)\n",
    "    val_probs_cal[b] = safe_softmax(val_logits[b], T=Tvec)\n",
    "    test_probs_cal[b] = safe_softmax(test_logits[b], T=Tvec)\n",
    "print(\"Class-wise temperatures ready.\")\n",
    "\n",
    "raw_scaler = StandardScaler().fit(X_train)\n",
    "Z_tr, Z_va, Z_te = raw_scaler.transform(X_train), raw_scaler.transform(X_val), raw_scaler.transform(X_test)\n",
    "class_means = np.vstack([Z_tr[y_train==c].mean(0) for c in range(n_classes)])\n",
    "class_invDiag = np.vstack([1.0 / (Z_tr[y_train==c].var(0) + 1e-3) for c in range(n_classes)])\n",
    "def mahalanobis_diag(Z):\n",
    "    return np.sqrt(np.stack([np.einsum('ij,ij->i', Z - mu, (Z - mu) * inv_diag) for mu, inv_diag in zip(class_means, class_invDiag)], axis=1))\n",
    "D_tr_maha, D_va_maha, D_te_maha = mahalanobis_diag(Z_tr), mahalanobis_diag(Z_va), mahalanobis_diag(Z_te)\n",
    "\n",
    "def build_meta(prob_map, raw_X_unscaled, D_maha):\n",
    "    blocks = [prob_map[b] for b in all_base_names]\n",
    "    for b in all_base_names: blocks.extend(entropy_and_margin(_row_norm(prob_map[b])))\n",
    "    return np.hstack(blocks + [raw_scaler.transform(raw_X_unscaled), D_maha])\n",
    "\n",
    "X_meta_train, X_meta_val, X_meta_test = build_meta(oof_probs_cal, X_train, D_tr_maha), build_meta(val_probs_cal, X_val, D_va_maha), build_meta(test_probs_cal, X_test, D_te_maha)\n",
    "w_train = 1.0 + (np.mean([oof_probs_cal[b] for b in all_base_names], axis=0).argmax(axis=1) != y_train).astype(float)\n",
    "\n",
    "cands = []\n",
    "def eval_meta(model):\n",
    "    model.fit(X_meta_train, y_train, sample_weight=w_train)\n",
    "    return -log_loss(y_val, _row_norm(model.predict_proba(X_meta_val)), labels=np.arange(n_classes)), model\n",
    "\n",
    "for C in [0.5, 1.0, 2.0]: cands.append((\"meta_logreg\", *eval_meta(LogisticRegression(C=C, multi_class=\"multinomial\", max_iter=5000, random_state=rng)), {\"C\": C}))\n",
    "cands.append((\"meta_xgb\", *eval_meta(XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.1, random_state=rng, use_label_encoder=False, eval_metric='mlogloss')), {}))\n",
    "if HAS_CAT: cands.append((\"meta_cat\", *eval_meta(CatBoostClassifier(iterations=200, depth=4, random_seed=rng, verbose=False)), {}))\n",
    "\n",
    "if not cands:\n",
    "    raise RuntimeError(\"No meta-learner candidates could be trained. Halting execution.\")\n",
    "\n",
    "best_name, _, best_model, _ = sorted(cands, key=lambda t: t[1], reverse=True)[0]\n",
    "print(f\"Best meta-learner on validation: {best_name}\")\n",
    "best_model.fit(np.vstack([X_meta_train, X_meta_val]), np.concatenate([y_train, y_val]), sample_weight=np.concatenate([w_train, np.ones(len(y_val))]))\n",
    "P_test_meta = _row_norm(best_model.predict_proba(X_meta_test))\n",
    "acc_meta, f1_meta = accuracy_score(y_test, P_test_meta.argmax(1)), f1_score(y_test, P_test_meta.argmax(1), average=\"macro\")\n",
    "\n",
    "def blend_logloss(W_flat, P_list, y_true):\n",
    "    W = np.maximum(W_flat.reshape(len(all_base_names), n_classes), 0)\n",
    "    W /= (W.sum(axis=0, keepdims=True) + 1e-12)\n",
    "    P = sum(P_list[b] * W[b, :] for b in range(len(all_base_names)))\n",
    "    return log_loss(y_true, _row_norm(P), labels=np.arange(n_classes))\n",
    "\n",
    "res = minimize(blend_logloss, x0=np.ones(len(all_base_names) * n_classes), args=([val_probs_cal[b] for b in all_base_names], y_val), method='L-BFGS-B')\n",
    "W_opt = np.maximum(res.x.reshape(len(all_base_names), n_classes), 0)\n",
    "W_opt /= (W_opt.sum(axis=0, keepdims=True) + 1e-12)\n",
    "\n",
    "test_probs_list = [test_probs_cal[b] for b in all_base_names]\n",
    "P_test_blend = sum(test_probs_list[b] * W_opt[b, :] for b in range(len(all_base_names)))\n",
    "acc_blend, f1_blend = accuracy_score(y_test, P_test_blend.argmax(1)), f1_score(y_test, P_test_blend.argmax(1), average=\"macro\")\n",
    "\n",
    "\n",
    "if acc_meta > acc_blend:\n",
    "    winner_name, winner_acc, winner_f1, winner_pred, winner_P = \"meta\", acc_meta, f1_meta, P_test_meta.argmax(1), P_test_meta\n",
    "else:\n",
    "    winner_name, winner_acc, winner_f1, winner_pred, winner_P = \"blend\", acc_blend, f1_blend, P_test_blend.argmax(1), P_test_blend\n",
    "print(f\"\\nWINNER: {winner_name.upper()} with Accuracy = {winner_acc:.4f}, F1-Macro = {winner_f1:.4f}\")\n",
    "\n",
    "plot_confusion_matrix(confusion_matrix(y_test, winner_pred), classes, os.path.join(OUT_DIR, \"cm_final.png\"), \"Confusion Matrix - Ensemble Final Version (Test)\")\n",
    "plot_roc_pr_curves_and_save_points(label_binarize(y_test, classes=np.arange(n_classes)), winner_P, classes, OUT_DIR, \"ROC - Ensemble Final Version (Test)\")\n",
    "\n",
    "with open(REPORT_JSON, \"w\") as f:\n",
    "    json.dump({\n",
    "        \"winner\": {\"name\": winner_name, \"accuracy\": winner_acc, \"f1_macro\": winner_f1},\n",
    "        \"classification_report\": classification_report(y_test, winner_pred, target_names=classes, output_dict=True)\n",
    "    }, f, indent=4)\n",
    "print(f\"\\nSaved all final artifacts to: {OUT_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8106950,
     "sourceId": 12820038,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
